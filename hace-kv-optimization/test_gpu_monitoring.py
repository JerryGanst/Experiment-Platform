#!/usr/bin/env python3
"""
GPUç›‘æ§è¯Šæ–­æµ‹è¯•è„šæœ¬
"""

import torch
import time
import json

def test_gpu_monitoring():
    """æµ‹è¯•GPUç›‘æ§åŠŸèƒ½"""
    print("ğŸ” å¼€å§‹GPUç›‘æ§è¯Šæ–­...")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUç›‘æ§")
        return
    
    print(f"GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        props = torch.cuda.get_device_properties(i)
        print(f"GPU {i}: {props.name}, æ€»æ˜¾å­˜: {props.total_memory / 1024**3:.1f}GB")
    
    # é‡ç½®å†…å­˜ç»Ÿè®¡
    torch.cuda.reset_peak_memory_stats(0)
    print("\né‡ç½®GPUå†…å­˜ç»Ÿè®¡...")
    
    # åˆå§‹å†…å­˜çŠ¶æ€
    print(f"åˆå§‹GPUå†…å­˜çŠ¶æ€:")
    print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
    print(f"  å·²ä¿ç•™: {torch.cuda.memory_reserved(0) / 1024**2:.1f}MB")
    print(f"  å³°å€¼å·²åˆ†é…: {torch.cuda.max_memory_allocated(0) / 1024**2:.1f}MB")
    print(f"  å³°å€¼å·²ä¿ç•™: {torch.cuda.max_memory_reserved(0) / 1024**2:.1f}MB")
    
    # åŠ è½½ä¸€ä¸ªç®€å•çš„æ¨¡å‹æ¥ä½¿ç”¨GPUå†…å­˜
    print("\nğŸš€ åŠ è½½ç®€å•æ¨¡å‹...")
    try:
        # åˆ›å»ºä¸€äº›å¼ é‡æ¥å ç”¨GPUå†…å­˜
        device = torch.device("cuda:0")
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)
        
        print(f"åˆ›å»ºå¼ é‡åGPUå†…å­˜çŠ¶æ€:")
        print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
        print(f"  å·²ä¿ç•™: {torch.cuda.memory_reserved(0) / 1024**2:.1f}MB")
        print(f"  å³°å€¼å·²åˆ†é…: {torch.cuda.max_memory_allocated(0) / 1024**2:.1f}MB")
        print(f"  å³°å€¼å·²ä¿ç•™: {torch.cuda.max_memory_reserved(0) / 1024**2:.1f}MB")
        
        # æ‰§è¡Œä¸€äº›æ“ä½œ
        z = torch.matmul(x, y)
        
        print(f"æ‰§è¡ŒçŸ©é˜µä¹˜æ³•åGPUå†…å­˜çŠ¶æ€:")
        print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
        print(f"  å·²ä¿ç•™: {torch.cuda.memory_reserved(0) / 1024**2:.1f}MB")
        print(f"  å³°å€¼å·²åˆ†é…: {torch.cuda.max_memory_allocated(0) / 1024**2:.1f}MB")
        print(f"  å³°å€¼å·²ä¿ç•™: {torch.cuda.max_memory_reserved(0) / 1024**2:.1f}MB")
        
        # æ¸…ç†å†…å­˜
        del x, y, z
        torch.cuda.empty_cache()
        
        print(f"æ¸…ç†åGPUå†…å­˜çŠ¶æ€:")
        print(f"  å·²åˆ†é…: {torch.cuda.memory_allocated(0) / 1024**2:.1f}MB")
        print(f"  å·²ä¿ç•™: {torch.cuda.memory_reserved(0) / 1024**2:.1f}MB")
        print(f"  å³°å€¼å·²åˆ†é…: {torch.cuda.max_memory_allocated(0) / 1024**2:.1f}MB")
        print(f"  å³°å€¼å·²ä¿ç•™: {torch.cuda.max_memory_reserved(0) / 1024**2:.1f}MB")
        
        # æµ‹è¯•ç›‘æ§æœŸé—´çš„å†…å­˜è·Ÿè¸ª
        print("\nğŸ“Š æµ‹è¯•æŒç»­ç›‘æ§...")
        
        memory_samples = []
        for i in range(10):
            # åˆ›å»ºå’Œé‡Šæ”¾å†…å­˜
            temp = torch.randn(500, 500, device=device)
            
            allocated = torch.cuda.memory_allocated(0) / 1024**2
            reserved = torch.cuda.memory_reserved(0) / 1024**2
            peak_allocated = torch.cuda.max_memory_allocated(0) / 1024**2
            peak_reserved = torch.cuda.max_memory_reserved(0) / 1024**2
            
            memory_samples.append({
                "iteration": i,
                "allocated_mb": allocated,
                "reserved_mb": reserved,
                "peak_allocated_mb": peak_allocated,
                "peak_reserved_mb": peak_reserved
            })
            
            del temp
            if i % 2 == 0:
                torch.cuda.empty_cache()
            
            time.sleep(0.1)
        
        print("å†…å­˜é‡‡æ ·ç»“æœ:")
        for sample in memory_samples:
            print(f"  è¿­ä»£{sample['iteration']}: åˆ†é…={sample['allocated_mb']:.1f}MB, å³°å€¼={sample['peak_allocated_mb']:.1f}MB")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        with open("hace-kv-optimization/gpu_monitoring_test_results.json", "w") as f:
            json.dump({
                "cuda_available": torch.cuda.is_available(),
                "device_count": torch.cuda.device_count(),
                "memory_samples": memory_samples,
                "final_peak_allocated": torch.cuda.max_memory_allocated(0) / 1024**2,
                "final_peak_reserved": torch.cuda.max_memory_reserved(0) / 1024**2
            }, f, indent=2)
        
        print("\nâœ… GPUç›‘æ§æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åˆ° gpu_monitoring_test_results.json")
        
    except Exception as e:
        print(f"âŒ GPUç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_gpu_monitoring() 