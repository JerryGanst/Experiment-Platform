2025-06-08 13:13:15,876 - __main__ - INFO - Starting CAKE experiment suite with run name: cake_run_20250608_131315
2025-06-08 13:13:15,876 - __main__ - INFO - Arguments: Namespace(model_name='meta-llama/Llama-2-7b-chat-hf', datasets='narrativeqa', kv_cache_lengths='512', batch_sizes='1', max_new_tokens=64, allocation_strategies='adaptive', cache_budgets='0.5', repetitions=1, output_dir='test_single', log_level='INFO', seed=42, run_name='cake_run_20250608_131315')
2025-06-08 13:13:15,877 - __main__ - INFO - Global EXPERIMENT_CONFIG being used: {'model_name_or_path': 'NousResearch/Llama-2-7b-hf', 'precision': 'fp16', 'datasets': ['mmlu', 'gsm8k', 'winogrande', 'arc_challenge', 'hellaswag', 'truthful_qa_mc'], 'dataset_subset_size': {'mmlu': 100, 'gsm8k': 100, 'winogrande': None, 'arc_challenge': None, 'hellaswag': None, 'truthful_qa_mc': None, 'pubmed_qa': 100, 'cais/mmlu-zh': 50}, 'kv_cache_lengths': [512, 1024, 2048], 'batch_sizes': [1, 4, 8], 'max_new_tokens': 256, 'repetitions': 3, 'h2o_enabled': True, 'h2o_ratios': [0.1, 0.2, 0.3], 'eviction_strategies': ['attention', 'time_decay', 'hybrid'], 'h2o_kv_cache_lengths': [512, 1024], 'cake_enabled': True, 'layer_allocation_strategies': ['uniform', 'adaptive', 'attention_based'], 'layer_analysis_configs': {'attention_pattern_analysis': True, 'layer_importance_scoring': True, 'dynamic_allocation': True}, 'cache_budgets': [0.5, 0.7, 0.9], 'cake_kv_cache_lengths': [512, 1024], 'head_level_optimization': False, 'head_analysis_enabled': False, 'head_selection_strategy': 'top_k', 'head_k_value': 10, 'output_base_dir': 'results', 'enable_monitoring': True, 'monitor_interval': 0.5}
2025-06-08 13:13:15,878 - __main__ - INFO - Global CAKE_MODEL_CONFIG being used: {'default_allocation_strategy': 'adaptive', 'default_cache_budget': 0.8, 'supported_models_cake': ['llama', 'mistral', 'qwen2'], 'dynamic_allocation_default': True, 'layer_analysis_default': {'attention_pattern_analysis': True, 'layer_importance_scoring': True}}
2025-06-08 13:13:15,898 - __main__ - INFO - Random seed set to 42
2025-06-08 13:13:15,898 - __main__ - INFO - Total number of CAKE experiment configurations to run: 1
2025-06-08 13:13:15,899 - __main__ - INFO - Running CAKE: Rep 1/1, Dataset: narrativeqa, KV_Len: 512, Batch: 1, Strategy: adaptive, Budget: 0.5
2025-06-08 13:13:15,900 - __main__ - INFO - Starting CAKE experiment: cake_narrativeqa_kv512_bs1_stratadaptive_bud0.5_rep0_20250608_131315
2025-06-08 13:13:15,901 - hace_core.utils.unified_monitor - INFO - 发现1个GPU设备
2025-06-08 13:13:15,901 - hace_core.utils.unified_monitor - INFO - 统一监控器初始化完成，实验ID: cake_narrativeqa_kv512_bs1_stratadaptive_bud0.5_rep0_20250608_131315
2025-06-08 13:13:15,901 - hace_core.utils.unified_monitor - INFO - 记录实验配置
2025-06-08 13:13:15,901 - __main__ - INFO - Loading model and tokenizer...
2025-06-08 13:13:15,901 - hace_core.models.model_loader - INFO - Loading model: meta-llama/Llama-2-7b-chat-hf
2025-06-08 13:13:17,028 - hace_core.models.model_loader - ERROR - Error loading model: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.
403 Client Error. (Request ID: Root=1-68451bf0-7251ac8a69388c3d626c9630;7cfa0b84-6017-4278-bfc9-30b4952b60af)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Your request to access model meta-llama/Llama-2-7b-chat-hf has been rejected by the repo's authors.
2025-06-08 13:13:17,029 - __main__ - ERROR - Error during CAKE experiment cake_narrativeqa_kv512_bs1_stratadaptive_bud0.5_rep0_20250608_131315: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.
403 Client Error. (Request ID: Root=1-68451bf0-7251ac8a69388c3d626c9630;7cfa0b84-6017-4278-bfc9-30b4952b60af)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Your request to access model meta-llama/Llama-2-7b-chat-hf has been rejected by the repo's authors.
Traceback (most recent call last):
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\utils\_http.py", line 409, in hf_raise_for_status
    response.raise_for_status()
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\requests\models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\transformers\utils\hub.py", line 470, in cached_files
    hf_hub_download(
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\file_download.py", line 961, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\file_download.py", line 1068, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\file_download.py", line 1596, in _raise_on_head_call_error
    raise head_call_error
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\file_download.py", line 1484, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\utils\_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\file_download.py", line 1401, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\file_download.py", line 285, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\file_download.py", line 309, in _request_wrapper
    hf_raise_for_status(response)
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\huggingface_hub\utils\_http.py", line 426, in hf_raise_for_status
    raise _format(GatedRepoError, message, response) from e
huggingface_hub.errors.GatedRepoError: 403 Client Error. (Request ID: Root=1-68451bf0-7251ac8a69388c3d626c9630;7cfa0b84-6017-4278-bfc9-30b4952b60af)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Your request to access model meta-llama/Llama-2-7b-chat-hf has been rejected by the repo's authors.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\JerryGanst\PycharmProjects\Experiment-Platform\hace-kv-optimization\baselines\cake_main.py", line 145, in run_cake_experiment
    model, tokenizer = load_model_and_tokenizer(current_model_config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\PycharmProjects\Experiment-Platform\hace-kv-optimization\hace_core\models\model_loader.py", line 31, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\transformers\models\auto\auto_factory.py", line 531, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\transformers\models\auto\configuration_auto.py", line 1133, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\transformers\configuration_utils.py", line 591, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\transformers\configuration_utils.py", line 650, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\transformers\utils\hub.py", line 312, in cached_file
    file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\JerryGanst\anaconda3\Lib\site-packages\transformers\utils\hub.py", line 533, in cached_files
    raise OSError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.
403 Client Error. (Request ID: Root=1-68451bf0-7251ac8a69388c3d626c9630;7cfa0b84-6017-4278-bfc9-30b4952b60af)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Your request to access model meta-llama/Llama-2-7b-chat-hf has been rejected by the repo's authors.
2025-06-08 13:13:17,036 - hace_core.utils.unified_monitor - ERROR - 实验标记为失败: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.
403 Client Error. (Request ID: Root=1-68451bf0-7251ac8a69388c3d626c9630;7cfa0b84-6017-4278-bfc9-30b4952b60af)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.
Your request to access model meta-llama/Llama-2-7b-chat-hf has been rejected by the repo's authors.
2025-06-08 13:13:17,037 - __main__ - INFO - Cleaned up resources for experiment cake_narrativeqa_kv512_bs1_stratadaptive_bud0.5_rep0_20250608_131315
2025-06-08 13:13:17,040 - __main__ - INFO - All CAKE experiment summaries saved to test_single\cake_run_20250608_131315\all_cake_experiments_summary.csv
2025-06-08 13:13:17,040 - __main__ - INFO - CAKE experiment suite finished.
