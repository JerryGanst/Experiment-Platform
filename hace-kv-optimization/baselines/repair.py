#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿé‡æ–°é›†æˆåŸºçº¿è¯„åˆ†ç³»ç»Ÿåˆ°fullkvcache_main.py
å®‰å…¨åœ°æ·»åŠ æ‰€æœ‰å¿…è¦çš„åŠŸèƒ½ï¼Œä¸ç ´ååŸæœ‰ä»£ç 
"""

import os
import re


def reintegrate_baseline_system():
    """é‡æ–°é›†æˆåŸºçº¿è¯„åˆ†ç³»ç»Ÿ"""
    file_path = 'fullkvcache_main.py'

    print("=== é‡æ–°é›†æˆåŸºçº¿è¯„åˆ†ç³»ç»Ÿ ===")

    # å¤‡ä»½åŸå§‹æ–‡ä»¶
    backup_path = file_path + '.before_reintegration'
    with open(file_path, 'r', encoding='utf-8') as f:
        original_content = f.read()

    with open(backup_path, 'w', encoding='utf-8') as f:
        f.write(original_content)
    print(f"ğŸ“¦ åŸå§‹æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_path}")

    content = original_content

    # 1. æ·»åŠ eval_utilså¯¼å…¥ï¼ˆåœ¨å…¶ä»–å¯¼å…¥åï¼‰
    eval_utils_import = '''
# å¯¼å…¥åŸºçº¿è¯„åˆ†å·¥å…·
try:
    eval_utils_path = os.path.join(os.path.dirname(__file__), '..')
    if eval_utils_path not in sys.path:
        sys.path.append(eval_utils_path)

    from eval_utils import (
        score_dataset,
        calculate_relative_score,
        aggregate_scores,
        format_score_report
    )
    BASELINE_SCORING_AVAILABLE = True
    print("[OK] åŸºçº¿è¯„åˆ†å·¥å…·åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"[WARNING] åŸºçº¿è¯„åˆ†å·¥å…·åŠ è½½å¤±è´¥: {e}")
    BASELINE_SCORING_AVAILABLE = False
'''

    # åœ¨importè¯­å¥åæ·»åŠ eval_utilså¯¼å…¥
    import_pattern = r'(from hace_core.*?\n)'
    if re.search(import_pattern, content):
        content = re.sub(import_pattern, r'\1' + eval_utils_import, content, count=1)
        print("âœ… æ·»åŠ äº†eval_utilså¯¼å…¥")

    # 2. ä¿®æ”¹æ•°æ®è·¯å¾„ï¼ˆå®‰å…¨çš„æ–¹å¼ï¼‰
    content = content.replace('data_dir="../../data"', 'data_dir="../data"')
    content = content.replace("data_dir='../../data'", "data_dir='../data'")
    print("âœ… ä¿®å¤äº†æ•°æ®è·¯å¾„")

    # 3. æ·»åŠ è¯„åˆ†å‚æ•°åˆ°argparse
    argparse_addition = '''    parser.add_argument("--enable_scoring", action="store_true", help="Enable scoring evaluation")
    parser.add_argument("--is_baseline_run", action="store_true", help="Mark this as a baseline run for establishing Full KV baseline scores")
'''

    # åœ¨ç°æœ‰å‚æ•°åæ·»åŠ æ–°å‚æ•°
    if 'add_argument' in content and '--enable_scoring' not in content:
        # æ‰¾åˆ°æœ€åä¸€ä¸ªadd_argumentçš„ä½ç½®
        last_arg_match = None
        for match in re.finditer(r'parser\.add_argument\([^)]+\)', content):
            last_arg_match = match

        if last_arg_match:
            insert_pos = last_arg_match.end()
            content = content[:insert_pos] + '\n' + argparse_addition + content[insert_pos:]
            print("âœ… æ·»åŠ äº†è¯„åˆ†å‚æ•°")

    # 4. æ·»åŠ åŸºçº¿å¤„ç†é€»è¾‘ï¼ˆåœ¨mainå‡½æ•°ç»“æŸå‰ï¼‰
    baseline_processing = '''
    # å¤„ç†åŸºçº¿è¯„åˆ†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.enable_scoring and args.is_baseline_run and BASELINE_SCORING_AVAILABLE:
        try:
            logger.info("å¼€å§‹å¤„ç†åŸºçº¿è¯„åˆ†...")

            # æ”¶é›†æ‰€æœ‰å®éªŒçš„è¯„åˆ†ç»“æœï¼Œå»ºç«‹åŸºçº¿
            baseline_scores = []

            for result in all_results_summary:
                if isinstance(result, dict) and 'experiment_id' in result:
                    # æŸ¥æ‰¾å¯¹åº”çš„è¯„åˆ†æ–‡ä»¶
                    experiment_id = result['experiment_id']

                    # ä»å®éªŒIDä¸­æå–æ•°æ®é›†åç§°
                    if 'ds_' in experiment_id:
                        dataset_part = experiment_id.split('ds_')[1].split('_')[0]

                        # æŸ¥æ‰¾è¯„åˆ†ç»“æœæ–‡ä»¶
                        for root, dirs, files in os.walk(main_output_dir):
                            for file in files:
                                if file.startswith(f"evaluation_results_") and experiment_id in file:
                                    eval_file_path = os.path.join(root, file)
                                    try:
                                        with open(eval_file_path, 'r', encoding='utf-8') as f:
                                            eval_data = json.load(f)
                                            if eval_data.get("average_score") is not None:
                                                score_result = calculate_relative_score(
                                                    dataset_name=dataset_part,
                                                    raw_score=eval_data["average_score"],
                                                    is_full_kv=True
                                                )
                                                baseline_scores.append(score_result)
                                                logger.info(f"åŸºçº¿åˆ†æ•°å·²è®°å½•: {dataset_part} = {eval_data['average_score']:.4f}")
                                    except Exception as e:
                                        logger.warning(f"å¤„ç†è¯„åˆ†æ–‡ä»¶æ—¶å‡ºé”™ {eval_file_path}: {e}")

            if baseline_scores:
                # ç”ŸæˆåŸºçº¿æŠ¥å‘Š
                aggregated = aggregate_scores(baseline_scores)
                report = format_score_report(aggregated, "Full KV (åŸºçº¿)")

                # ä¿å­˜åŸºçº¿æŠ¥å‘Š
                baseline_report_path = os.path.join(main_output_dir, "baseline_scoring_report.txt")
                with open(baseline_report_path, 'w', encoding='utf-8') as f:
                    f.write(report)

                logger.info(f"åŸºçº¿è¯„åˆ†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {baseline_report_path}")
                print(report)
            else:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¯„åˆ†ç»“æœï¼Œæ— æ³•å»ºç«‹åŸºçº¿")

        except Exception as baseline_error:
            logger.error(f"å¤„ç†åŸºçº¿è¯„åˆ†æ—¶å‡ºé”™: {baseline_error}")

    elif args.enable_scoring and not args.is_baseline_run:
        logger.info("è¯„åˆ†å·²å¯ç”¨ï¼Œä½†è¿™ä¸æ˜¯åŸºçº¿è¿è¡Œï¼Œè·³è¿‡åŸºçº¿å»ºç«‹")
'''

    # åœ¨mainå‡½æ•°ç»“æŸå‰æ·»åŠ åŸºçº¿å¤„ç†é€»è¾‘
    if 'logger.info("FullKVCache experiment suite finished.")' in content:
        content = content.replace(
            'logger.info("FullKVCache experiment suite finished.")',
            baseline_processing + '\n    logger.info("FullKVCache experiment suite finished.")'
        )
        print("âœ… æ·»åŠ äº†åŸºçº¿å¤„ç†é€»è¾‘")

    # 5. ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

    print("âœ… åŸºçº¿è¯„åˆ†ç³»ç»Ÿé‡æ–°é›†æˆå®Œæˆ")

    return True


def verify_integration():
    """éªŒè¯é›†æˆæ˜¯å¦æˆåŠŸ"""
    print("\n=== éªŒè¯é›†æˆç»“æœ ===")

    file_path = 'fullkvcache_main.py'

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æ£€æŸ¥å…³é”®åŠŸèƒ½
    checks = [
        ('eval_utilså¯¼å…¥', 'from eval_utils import'),
        ('åŸºçº¿è¯„åˆ†å¯ç”¨æ€§', 'BASELINE_SCORING_AVAILABLE'),
        ('è¯„åˆ†å‚æ•°', '--enable_scoring'),
        ('åŸºçº¿è¿è¡Œå‚æ•°', '--is_baseline_run'),
        ('ç›¸å¯¹è¯„åˆ†è®¡ç®—', 'calculate_relative_score'),
        ('åŸºçº¿å¤„ç†é€»è¾‘', 'å¼€å§‹å¤„ç†åŸºçº¿è¯„åˆ†'),
        ('æ•°æ®è·¯å¾„ä¿®å¤', '../data')
    ]

    for check_name, keyword in checks:
        if keyword in content:
            print(f"âœ… {check_name}: å·²é›†æˆ")
        else:
            print(f"âŒ {check_name}: ç¼ºå¤±")

    # æ£€æŸ¥è¯­æ³•
    try:
        import ast
        ast.parse(content)
        print("âœ… Pythonè¯­æ³•éªŒè¯é€šè¿‡")
    except SyntaxError as e:
        print(f"âŒ è¯­æ³•é”™è¯¯: {e}")
        return False

    return True


if __name__ == "__main__":
    print("ğŸš€ å¿«é€Ÿé‡æ–°é›†æˆåŸºçº¿è¯„åˆ†ç³»ç»Ÿ")
    print("=" * 50)

    success = reintegrate_baseline_system()

    if success:
        integration_ok = verify_integration()

        if integration_ok:
            print("\nğŸ‰ åŸºçº¿è¯„åˆ†ç³»ç»Ÿé‡æ–°é›†æˆæˆåŠŸï¼")
            print("\nğŸ“‹ ç°åœ¨å¯ä»¥æµ‹è¯•:")
            print(
                "python hace-kv-optimization/baselines/fullkvcache_main.py --enable_scoring --is_baseline_run --datasets hotpotqa --kv_cache_lengths 128 --repetitions 1")
        else:
            print("\nâš ï¸ é›†æˆå®Œæˆä½†å¯èƒ½æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥")
    else:
        print("\nâŒ é‡æ–°é›†æˆå¤±è´¥")