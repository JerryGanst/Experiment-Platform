# FullKVCache实验主脚本 - 完全不使用任何缓存优化

import sys
import os

# 设置调试和内存管理环境变量
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'  # 设备端断言支持
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# 获取当前文件的绝对路径
current_file_path = os.path.abspath(__file__)
# 获取项目目录的路径
project_dir = os.path.dirname(current_file_path)
# 获取项目根目录的路径
project_root_dir = os.path.dirname(project_dir)

# 如果项目根目录不在 sys.path 中，则添加它
if project_root_dir not in sys.path:
    sys.path.insert(0, project_root_dir)

"""
FullKVCache实验执行脚本 - 使用完整KV缓存，不进行任何优化
修复版：解决CUDA设备端断言错误和内存累积问题
支持本地JSONL数据文件
"""
import time
import logging
import argparse
import json
import torch
import random
import numpy as np
import pandas as pd
import gc
from tqdm import tqdm
from datetime import datetime
from transformers import LogitsProcessor, LogitsProcessorList

# 导入项目模块
from hace_core import config
MODEL_CONFIG = config.MODEL_CONFIG
EXPERIMENT_CONFIG = config.EXPERIMENT_CONFIG
DATASET_CONFIG = config.DATASET_CONFIG
OUTPUT_CONFIG = config.OUTPUT_CONFIG
MONITORING_CONFIG = config.MONITORING_CONFIG

# 导入模块 - 更新路径以匹配新的目录结构
from hace_core.models.model_loader import (
    load_model_and_tokenizer,
    configure_model_for_kv_cache_length,
    prepare_model_for_baseline
)
from hace_core.data.dataset_loader import load_dataset_split, prepare_samples_for_evaluation, prepare_batch
from hace_core.utils.unified_monitor import UnifiedMonitor

# 导入评分模块
try:
    longbench_metrics_path = os.path.join(os.path.dirname(__file__), '..', 'cakekv-main', 'cakekv-main', 'experiments', 'LongBench')
    if longbench_metrics_path not in sys.path:
        sys.path.append(longbench_metrics_path)
    
    from metrics import (
        qa_f1_score, rouge_score, classification_score, 
        retrieval_score, count_score, code_sim_score,
        normalize_answer
    )
    SCORING_AVAILABLE = True
    print("[OK] 评分模块加载成功")
except ImportError as e:
    print(f"[WARNING] 评分模块加载失败: {e}")
    SCORING_AVAILABLE = False

# 导入新的基线评分工具
try:
    eval_utils_path = os.path.join(os.path.dirname(__file__), '..')
    if eval_utils_path not in sys.path:
        sys.path.append(eval_utils_path)
    
    from eval_utils import (
        score_dataset,
        calculate_relative_score,
        aggregate_scores,
        format_score_report
    )
    BASELINE_SCORING_AVAILABLE = True
    print("[OK] 基线评分工具加载成功")
except ImportError as e:
    print(f"[WARNING] 基线评分工具加载失败: {e}")
    BASELINE_SCORING_AVAILABLE = False

# 数据集评分映射
DATASET_SCORING_MAP = {
    "mmlu": qa_f1_score,
    "narrativeqa": qa_f1_score,
    "qasper": qa_f1_score,
    "multifieldqa_en": qa_f1_score,
    "hotpotqa": qa_f1_score,
    "2wikimqa": qa_f1_score,
    "musique": qa_f1_score,
    "gov_report": rouge_score,
    "qmsum": rouge_score,
    "multi_news": rouge_score,
    "trec": classification_score,
    "triviaqa": qa_f1_score,
    "samsum": rouge_score,
    "passage_retrieval_en": retrieval_score,
    "passage_count": count_score,
    "lcc": code_sim_score,
    "repobench-p": code_sim_score,
}


def comprehensive_cleanup():
    """
    全面的CUDA内存清理 - 修复版本
    解决KV-cache实验中的内存累积问题
    """
    try:
        # 强制垃圾回收
        gc.collect()
        
        # 清理CUDA缓存和上下文
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            torch.cuda.synchronize()
            
            # 重置内存统计
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()
            
            # 打印内存状态用于调试
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            if allocated > 0 or reserved > 0:
                print(f"GPU内存状态 - 已分配: {allocated:.2f}GB, 已保留: {reserved:.2f}GB")
                
    except Exception as cleanup_error:
        print(f"清理过程中出现警告: {cleanup_error}")


def validate_kv_cache_inputs(queries, keys, values):
    """
    验证KV-cache输入以防止索引越界和数值错误
    """
    try:
        # 检查张量维度匹配
        if queries.shape[-1] != keys.shape[-1]:
            raise ValueError(f"Q-K维度不匹配: {queries.shape[-1]} vs {keys.shape[-1]}")
        
        # 检查NaN值
        if torch.isnan(queries).any():
            raise ValueError("查询张量中存在NaN值")
        if torch.isnan(keys).any():
            raise ValueError("键张量中存在NaN值")
        if torch.isnan(values).any():
            raise ValueError("值张量中存在NaN值")
            
        # 检查无穷值
        if torch.isinf(queries).any():
            raise ValueError("查询张量中存在无穷值")
        if torch.isinf(keys).any():
            raise ValueError("键张量中存在无穷值")
        if torch.isinf(values).any():
            raise ValueError("值张量中存在无穷值")
            
        return True
    except Exception as e:
        logger.error(f"输入验证失败: {e}")
        return False


def monitor_memory():
    """
    实时内存监控和碎片化检测
    """
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        fragmentation = (reserved - allocated) / reserved if reserved > 0 else 0
        
        print(f"GPU内存监控 - 已分配: {allocated:.2f}GB, "
              f"已保留: {reserved:.2f}GB, "
              f"碎片化率: {fragmentation:.2%}")
        
        if fragmentation > 0.3:  # 30%碎片化阈值
            print("⚠️ 内存碎片化严重，执行清理")
            comprehensive_cleanup()
            
        return allocated, reserved, fragmentation
    return 0, 0, 0


def evaluate_response_quality(prediction, ground_truth, dataset_name, all_classes=None):
    """
    评估回答质量
    
    Args:
        prediction: 模型生成的回答
        ground_truth: 标准答案（可能是列表）
        dataset_name: 数据集名称
        all_classes: 分类任务的所有类别
    
    Returns:
        score: 评分结果 (0-1之间)
    """
    if not SCORING_AVAILABLE:
        return None
    
    # 获取评分函数
    scoring_function = DATASET_SCORING_MAP.get(dataset_name)
    if not scoring_function:
        logger.warning(f"数据集 {dataset_name} 暂不支持自动评分")
        return None
    
    try:
        # 处理多个标准答案的情况
        if isinstance(ground_truth, list):
            scores = []
            for gt in ground_truth:
                score = scoring_function(prediction, gt, all_classes=all_classes)
                scores.append(score)
            return max(scores)  # 取最高分
        else:
            return scoring_function(prediction, ground_truth, all_classes=all_classes)
    except Exception as e:
        logger.error(f"评分时出错: {e}")
        return None


def extract_ground_truth_from_sample(sample, dataset_source):
    """修复版：直接使用正确的答案字段"""
    print(f"[DEBUG] 提取答案 - 样本键: {list(sample.keys())}")
    
    # 优先使用我们手动设置的reference字段
    if 'reference' in sample and sample['reference']:
        reference = sample['reference']
        print(f"[DEBUG] 使用reference字段: {reference}")
        if isinstance(reference, list):
            return [str(item) for item in reference]
        else:
            return [str(reference)]
    
    # 回退到原始样本
    if 'original_sample' in sample:
        original = sample['original_sample']
        if 'answers' in original:
            answers = original['answers']
            print(f"[DEBUG] 使用原始样本answers: {answers}")
            if isinstance(answers, list):
                return [str(item) for item in answers]
            else:
                return [str(answers)]
    
    # 其他字段检查...
    answer_fields = ['answers', 'answer', 'output', 'gold', 'target', 'label']
    for field in answer_fields:
        if field in sample and sample[field]:
            value = sample[field]
            print(f"[DEBUG] 找到字段 '{field}': {value}")
            if isinstance(value, list):
                return [str(item) for item in value]
            else:
                return [str(value)]
    
    print(f"[DEBUG] 未找到答案，完整样本: {sample}")
    return ["Unknown"]


def load_local_jsonl(dataset_name, data_dir="../../data"):
    """
    从本地JSONL文件加载数据集
    
    Args:
        dataset_name: 数据集名称
        data_dir: 数据目录路径
        
    Returns:
        dataset: 数据列表
    """
    file_path = os.path.join(data_dir, f"{dataset_name}.jsonl")
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"❌ 本地文件不存在: {file_path}")

    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:  # 跳过空行
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    logger.warning(f"跳过无效的JSON行: {line[:100]}... 错误: {e}")
    
    logger.info(f"✅ 从本地加载 {dataset_name}，共 {len(data)} 条样本")
    return data


def load_dataset_with_fallback(dataset_name, dataset_options, split="validation"):
    """
    加载数据集，优先使用Hugging Face，失败时回退到本地JSONL文件
    
    Args:
        dataset_name: 数据集名称
        dataset_options: 数据集配置选项
        split: 数据分割名称
        
    Returns:
        dataset: 加载的数据集
        source: 数据源 ("huggingface" 或 "local")
    """
    # 首先尝试从本地JSONL文件加载（优先级更高，确保使用带答案的验证集）
    try:
        logger.info(f"尝试从本地JSONL文件加载数据集: {dataset_name}")
        local_data = load_local_jsonl(dataset_name)
        
        # 创建一个简单的数据集对象，模拟datasets库的格式
        class SimpleDataset:
            def __init__(self, data):
                self.data = data
                
            def __len__(self):
                return len(self.data)
                
            def __getitem__(self, idx):
                return self.data[idx]
                
            def __iter__(self):
                return iter(self.data)
        
        dataset = SimpleDataset(local_data)
        logger.info(f"✅ 成功从本地JSONL文件加载 {dataset_name} (来源: local)")
        return dataset, "local"
    except Exception as local_error:
        logger.warning(f"⚠️ 无法从本地加载 {dataset_name}: {local_error}")
        logger.info(f"回退到从Hugging Face加载数据集: {dataset_name}")
        try:
            # 回退到Hugging Face加载
            dataset = load_dataset_split(dataset_options, split=split)
            logger.info(f"✅ 成功从Hugging Face加载 {dataset_name} (来源: huggingface)")
            return dataset, "huggingface"
        except Exception as hf_error:
            logger.error(f"❌ 无法从Hugging Face加载 {dataset_name}: {hf_error}")
            raise Exception(f"无法从任何来源加载数据集 {dataset_name}。本地错误: {local_error}. Hugging Face错误: {hf_error}")


# 设置日志
def setup_logging(log_file=None, level=logging.INFO):
    """设置日志记录"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    log_dir = None
    if log_file:
        log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)

    handlers = []
    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    handlers.append(logging.StreamHandler())

    logging.basicConfig(
        level=level,
        format=log_format,
        handlers=handlers,
        force=True  # 强制重新配置日志
    )
    # 减少一些库的日志输出
    logging.getLogger("transformers").setLevel(logging.WARNING)
    logging.getLogger("datasets").setLevel(logging.WARNING)

    return logging.getLogger(__name__)

logger = logging.getLogger(__name__)

def set_seed(seed):
    """设置随机种子以确保可重现性"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    logger.info(f"Random seed set to {seed}")


def run_fullkvcache_experiment(model_config, dataset_name, dataset_options,
                              kv_cache_length, batch_size, max_new_tokens,
                              output_dir, repeat_index=0):
    """
    运行单次FullKVCache实验 - 修复版
    添加全面的内存管理和错误处理

    Args:
        model_config: 模型配置
        dataset_name: 数据集名称
        dataset_options: 数据集配置选项 
        kv_cache_length: KV缓存长度
        batch_size: 批处理大小
        max_new_tokens: 最大生成令牌数
        output_dir: 输出目录
        repeat_index: 重复实验的索引

    Returns:
        metrics: 性能指标
    """
    run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    experiment_id = f"fullkvcache_{dataset_name}_kv{kv_cache_length}_bs{batch_size}_rep{repeat_index}_{run_timestamp}"
    logger.info(f"Starting FullKVCache experiment: {experiment_id}")

    # 实验前清理
    comprehensive_cleanup()
    logger.info("实验前内存清理完成")

    # 初始化统一监控器
    monitor = UnifiedMonitor(experiment_id=experiment_id)
    monitor.record_config({
        "model_name": model_config["model_name_or_path"],
        "precision": model_config["precision"],
        "batch_size": batch_size,
        "kv_cache_length": kv_cache_length,
        "max_new_tokens": max_new_tokens,
        "use_fullkvcache": True,
        "dataset": dataset_name,
        "repetition": repeat_index
    })

    # 初始化变量
    model = None
    tokenizer = None
    inputs = None
    outputs = None
    try:
        # 加载模型和分词器
        logger.info("Loading model and tokenizer...")
        monitor_memory()  # 监控加载前的内存状态
        
        model, tokenizer = load_model_and_tokenizer(model_config)
        logger.info(f"模型加载后GPU内存: {torch.cuda.memory_allocated()/1e9:.2f}GB")

        # 配置模型的KV缓存长度，但不进行任何优化
        model = configure_model_for_kv_cache_length(model, kv_cache_length)

        # 准备基线模型（完整KV缓存，无优化）
        model = prepare_model_for_baseline(model)
        
        # 确保使用完整缓存
        model.config.use_cache = True
        
        # RTX 4090特定优化
        if torch.cuda.is_available():
            torch.backends.cuda.max_split_size_mb = 128
            torch.backends.cudnn.benchmark = True

        # 加载数据集，使用新的回退机制
        logger.info(f"Loading dataset {dataset_name}...")
        dataset, dataset_source = load_dataset_with_fallback(dataset_name, dataset_options, split="test")

        # 准备评估样本（为了评分，使用较小的样本数）
        num_eval_samples = EXPERIMENT_CONFIG.get("dataset_subset_size", {}).get(dataset_name)
        if num_eval_samples is None:
            num_eval_samples = min(20, len(dataset))  # 减少到20个样本以便评分
        
        actual_num_samples_to_prepare = min(batch_size, num_eval_samples)
        if actual_num_samples_to_prepare == 0:
            error_msg = f"没有足够的样本进行实验 (需要 {batch_size}, 可用 {num_eval_samples})。"
            logger.error(error_msg)
            monitor.mark_failure(error_msg)
            return monitor.get_comprehensive_metrics()

        # 临时调试：检查预处理前后的样本
        print(f"[DEBUG] 原始数据集第一个样本: {dataset[0] if hasattr(dataset, '__getitem__') else 'No direct access'}")
        
        # 🔧 绕过有问题的预处理，直接使用原始数据
        print("🔧 绕过有问题的预处理，直接使用原始数据")
        samples = []
        for i in range(min(actual_num_samples_to_prepare, len(dataset))):
            original_sample = dataset[i]
            # 手动创建正确的样本格式
            processed_sample = {
                'prompt': original_sample['input'],
                'reference': original_sample['answers'] if isinstance(original_sample['answers'], list) else [original_sample['answers']],
                'context': original_sample.get('context', ''),
                'original_sample': original_sample  # 保留原始样本用于调试
            }
            samples.append(processed_sample)
            print(f"[DEBUG] 手动处理样本 {i+1}: prompt={processed_sample['prompt'][:50]}...")
            print(f"[DEBUG] 手动处理样本 {i+1}: reference={processed_sample['reference']}")
        
        print(f"[DEBUG] 预处理后样本: {samples[0] if samples else 'No samples'}")

        # 准备批处理
        effective_max_length = min(kv_cache_length, model.config.max_position_embeddings)
        logger.info(f"Preparing batch with size {batch_size}, max_length {effective_max_length}...")
        
        batch = prepare_batch(
            samples,
            tokenizer,
            batch_size=actual_num_samples_to_prepare,
            max_length=effective_max_length
        )

        # 将批处理数据移至设备
        inputs = {
            "input_ids": batch["input_ids"].to(model.device),
            "attention_mask": batch["attention_mask"].to(model.device)
        }
        if "token_type_ids" in batch and batch["token_type_ids"] is not None:
             inputs["token_type_ids"] = batch["token_type_ids"].to(model.device)
        
        # 验证输入数据
        for key, tensor in inputs.items():
            if torch.isnan(tensor).any():
                raise ValueError(f"输入数据 {key} 包含NaN值")
            if torch.isinf(tensor).any():
                raise ValueError(f"输入数据 {key} 包含无穷值")
            if tensor.max() >= model.config.vocab_size and key == "input_ids":
                raise ValueError(f"输入token ID超出词汇表范围: {tensor.max()} >= {model.config.vocab_size}")
        
        logger.info("输入数据验证通过")

        # 预热（可选）
        logger.info("Warming up FullKVCache model...")
        with torch.no_grad():
            _ = model.generate(
                **inputs,
                max_new_tokens=min(5, max_new_tokens),
                do_sample=False,
                use_cache=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # 清理GPU缓存
        torch.cuda.empty_cache()
        time.sleep(0.5)

        # 启动统一监控
        if EXPERIMENT_CONFIG.get("enable_monitoring", True):
            monitor.start_monitoring()

        # 开始性能测量
        logger.info("Starting FullKVCache performance measurement...")
        monitor.start_generation()

        # 定义自定义 LogitsProcessor 来记录令牌生成时间
        class TokenTimeLogitsProcessor(LogitsProcessor):
            def __init__(self, monitor):
                self.monitor = monitor
                self.first_token_recorded = False

            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):
                if not self.first_token_recorded:
                    self.monitor.record_first_token()
                    self.first_token_recorded = True
                else:
                    self.monitor.record_token()
                return scores

        token_time_processor = TokenTimeLogitsProcessor(monitor)
        logits_processor_list = LogitsProcessorList([token_time_processor])

        # 生成配置
        generate_kwargs = DATASET_CONFIG.get("generate_config", {}).copy()
        generate_kwargs.update({
            "max_new_tokens": max_new_tokens,
            "logits_processor": logits_processor_list,
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            "use_cache": True  # 明确启用完整缓存
        })

        # 生成文本
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **generate_kwargs
            )

        # 结束性能测量
        monitor.end_generation()

        # 自动评分处理
        evaluation_results = []
        total_score = 0.0
        scored_samples = 0
        
        if SCORING_AVAILABLE:
            logger.info("开始自动评分...")
            try:
                # 解码生成的文本
                input_length = inputs["input_ids"].shape[1]
                generated_tokens = outputs[:, input_length:]
                
                for i in range(generated_tokens.shape[0]):
                    try:
                        generated_text = tokenizer.decode(generated_tokens[i], skip_special_tokens=True)
                        
                        # 获取对应的原始样本和标准答案
                        if i < len(samples):
                            sample = samples[i]
                            ground_truth = extract_ground_truth_from_sample(sample, dataset_source)
                            
                            # 计算分数
                            score = evaluate_response_quality(generated_text, ground_truth, dataset_name)
                            
                            if score is not None:
                                total_score += score
                                scored_samples += 1
                                evaluation_results.append({
                                    "sample_id": i,
                                    "prediction": generated_text[:500],  # 限制长度
                                    "ground_truth": str(ground_truth)[:200] if ground_truth else "Unknown",
                                    "score": score
                                })
                                logger.info(f"样本 {i+1} 评分: {score:.3f}")
                        
                    except Exception as e:
                        logger.warning(f"评分样本 {i+1} 时出错: {e}")
                
                # 计算平均分数
                # 验证是否有有效的ground truth
                invalid_gt_count = sum(1 for result in evaluation_results if result.get('ground_truth') == "['Unknown']")
                if invalid_gt_count > 0:
                    logger.warning(f"⚠️ 发现 {invalid_gt_count} 个样本的ground truth为Unknown，评分可能无效")
                    logger.warning("请检查数据集格式和答案提取逻辑")
                
                if scored_samples > 0:
                    average_score = total_score / scored_samples
                    logger.info(f"✅ 评分完成! 平均分数: {average_score:.3f} ({scored_samples}/{len(samples)} 个样本)")
                    
                    # 将评分结果添加到监控指标中
                    monitor.performance_metrics["evaluation"] = {
                        "average_score": average_score,
                        "total_score": total_score,
                        "scored_samples": scored_samples,
                        "total_samples": len(samples),
                        "scoring_coverage": scored_samples / len(samples) if len(samples) > 0 else 0,
                        "individual_results": evaluation_results
                    }
                else:
                    logger.warning("⚠️ 没有成功评分的样本")
                    
            except Exception as e:
                logger.error(f"评分过程中出现错误: {e}")
        else:
            logger.info("评分模块不可用，跳过自动评分")

        # 停止监控并收集指标
        if EXPERIMENT_CONFIG.get("enable_monitoring", True):
            monitor.stop_monitoring()

        # 计算和保存指标
        final_metrics = monitor.get_comprehensive_metrics()
        monitor.save_metrics(output_dir, filename=f"fullkvcache_metrics_{experiment_id}.json")
        
        # 保存评分结果
        if evaluation_results:
            eval_file = os.path.join(output_dir, f"evaluation_results_{experiment_id}.json")
            try:
                with open(eval_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "experiment_id": experiment_id,
                        "dataset": dataset_name,
                        "average_score": average_score if scored_samples > 0 else 0,
                        "results": evaluation_results
                    }, f, indent=2, ensure_ascii=False)
                logger.info(f"评分结果已保存到: {eval_file}")
            except Exception as e:
                logger.error(f"保存评分结果时出错: {e}")
        
        logger.info(f"FullKVCache Experiment {experiment_id} completed. Metrics: {final_metrics}")
        return final_metrics

    except RuntimeError as e:
        error_msg = str(e)
        if "device-side assert" in error_msg:
            logger.error(f"检测到CUDA设备端断言错误: {error_msg}")
            logger.info("切换到CPU进行调试...")
            try:
                # 将模型切换到CPU获取详细错误信息
                if model is not None:
                    model_cpu = model.cpu()
                    if inputs is not None:
                        inputs_cpu = {k: v.cpu() for k, v in inputs.items()}
                        # 尝试在CPU上运行以获取真实错误
                        with torch.no_grad():
                            _ = model_cpu.generate(**inputs_cpu, max_new_tokens=5)
                    del model_cpu
            except Exception as cpu_error:
                logger.error(f"CPU调试显示真实错误: {cpu_error}")
        
        logger.error(f"运行时错误 - 实验 {experiment_id}: {error_msg}", exc_info=True)
        monitor.mark_failure(error_msg)
        return monitor.get_comprehensive_metrics()
    except Exception as e:
        logger.error(f"Error during FullKVCache experiment {experiment_id}: {e}", exc_info=True)
        monitor.mark_failure(str(e))
        return monitor.get_comprehensive_metrics()
    finally:
        # 全面清理模型和GPU内存
        try:
            logger.info(f"开始清理实验 {experiment_id} 的资源...")
            
            # 删除所有大对象
            if model is not None:
                del model
            if tokenizer is not None:
                del tokenizer
            if inputs is not None:
                del inputs
            if outputs is not None:
                del outputs
            
            # 执行全面清理
            comprehensive_cleanup()
            
            logger.info(f"实验 {experiment_id} 资源清理完成")
        except Exception as cleanup_error:
            logger.warning(f"清理过程中出现错误: {cleanup_error}")


def main():
    parser = argparse.ArgumentParser(description="Run FullKVCache Experiments")
    parser.add_argument("--model_name", type=str, default=EXPERIMENT_CONFIG["model_name_or_path"], help="Name or path of the model to use.")
    parser.add_argument("--datasets", type=str, default=",".join(EXPERIMENT_CONFIG["datasets"]), help="Comma-separated list of datasets to use.")
    parser.add_argument("--kv_cache_lengths", type=str, default=",".join(map(str, EXPERIMENT_CONFIG["kv_cache_lengths"])), help="Comma-separated list of KV cache lengths.")
    parser.add_argument("--batch_sizes", type=str, default=",".join(map(str, EXPERIMENT_CONFIG["batch_sizes"])), help="Comma-separated list of batch sizes.")
    parser.add_argument("--max_new_tokens", type=int, default=EXPERIMENT_CONFIG["max_new_tokens"], help="Maximum number of new tokens to generate.")
    parser.add_argument("--repetitions", type=int, default=EXPERIMENT_CONFIG["repetitions"], help="Number of repetitions for each experiment configuration.")
    parser.add_argument("--output_dir", type=str, default=os.path.join(EXPERIMENT_CONFIG["output_base_dir"], "fullkvcache_experiments"), help="Directory to save experiment results.")
    parser.add_argument("--log_level", type=str, default="INFO", help="Logging level (DEBUG, INFO, WARNING, ERROR)")
    parser.add_argument("--seed", type=int, default=config.EXPERIMENT_CONFIG.get("random_seed", 42), help="Random seed for reproducibility.")
    parser.add_argument("--run_name", type=str, default=f"fullkvcache_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}", help="A specific name for this run/sweep of experiments.")
    parser.add_argument("--enable_scoring", action="store_true", help="Enable scoring evaluation")
    parser.add_argument("--is_baseline_run", action="store_true", help="Mark this as a baseline run for establishing Full KV baseline scores")

    args = parser.parse_args()

    # 创建本次运行的总输出目录
    main_output_dir = os.path.join(args.output_dir, args.run_name)
    os.makedirs(main_output_dir, exist_ok=True)

    # 设置日志
    log_file_path = os.path.join(main_output_dir, "fullkvcache_experiment_log.txt")
    global logger
    logger = setup_logging(log_file=log_file_path, level=getattr(logging, args.log_level.upper(), logging.INFO))

    logger.info(f"Starting FullKVCache experiment suite with run name: {args.run_name}")
    logger.info(f"Arguments: {args}")
    logger.info(f"Global EXPERIMENT_CONFIG being used: {EXPERIMENT_CONFIG}")

    # 设置随机种子
    set_seed(args.seed)

    # 解析参数列表
    datasets_list = [d.strip() for d in args.datasets.split(',') if d.strip()]
    kv_lengths_list = [int(kv.strip()) for kv in args.kv_cache_lengths.split(',') if kv.strip()]
    batch_sizes_list = [int(bs.strip()) for bs in args.batch_sizes.split(',') if bs.strip()]

    all_results_summary = []
    total_experiments = len(datasets_list) * len(kv_lengths_list) * len(batch_sizes_list) * args.repetitions
    logger.info(f"Total number of FullKVCache experiment configurations to run: {total_experiments}")
    pbar = tqdm(total=total_experiments, desc="Running FullKVCache Experiments")

    current_model_config = {
        "model_name_or_path": args.model_name,
        "precision": EXPERIMENT_CONFIG["precision"]
    }

    for rep in range(args.repetitions):
        for dataset_name in datasets_list:
            # 获取数据集配置
            dataset_options = DATASET_CONFIG.get("available_datasets", {}).get(dataset_name)
            if not dataset_options:
                logger.error(f"Dataset configuration for '{dataset_name}' not found in DATASET_CONFIG. Skipping...")
                pbar.update(len(kv_lengths_list) * len(batch_sizes_list))
                continue
            
            for kv_len in kv_lengths_list:
                for bs in batch_sizes_list:
                    logger.info(f"Running FullKVCache: Rep {rep+1}/{args.repetitions}, Dataset: {dataset_name}, KV_Len: {kv_len}, Batch: {bs}")
                    
                    # 实验间内存清理
                    comprehensive_cleanup()
                    monitor_memory()
                    
                    # 为当前实验创建特定的输出子目录
                    exp_label = f"ds_{dataset_name}_kv{kv_len}_bs{bs}_rep{rep}"
                    current_exp_output_dir = os.path.join(main_output_dir, exp_label)
                    os.makedirs(current_exp_output_dir, exist_ok=True)

                    try:
                        experiment_metrics = run_fullkvcache_experiment(
                            model_config=current_model_config,
                            dataset_name=dataset_name,
                            dataset_options=dataset_options,
                            kv_cache_length=kv_len,
                            batch_size=bs,
                            max_new_tokens=args.max_new_tokens,
                            output_dir=current_exp_output_dir,
                            repeat_index=rep
                        )
                        all_results_summary.append(experiment_metrics)
                        logger.info(f"✓ 实验成功完成: {exp_label}")
                    except Exception as exp_error:
                        logger.error(f"✗ 实验失败: {exp_label}, 错误: {exp_error}")
                        # 记录失败的实验
                        failed_metrics = {
                            "experiment_id": exp_label,
                            "error": str(exp_error),
                            "status": "failed"
                        }
                        all_results_summary.append(failed_metrics)
                    finally:
                        # 确保每个实验后都清理
                        comprehensive_cleanup()
                    
                    pbar.update(1)
    pbar.close()

    # 保存所有实验结果的汇总
    summary_file_path = os.path.join(main_output_dir, "all_fullkvcache_experiments_summary.csv")
    if all_results_summary and isinstance(all_results_summary[0], dict):
        summary_df = pd.DataFrame(all_results_summary)
        summary_df.to_csv(summary_file_path, index=False)
        logger.info(f"All FullKVCache experiment summaries saved to {summary_file_path}")
    elif all_results_summary:
        logger.warning(f"Result summary items are not all dicts, cannot easily save to CSV. First item: {all_results_summary[0]}")
        summary_json_path = os.path.join(main_output_dir, "all_fullkvcache_experiments_summary.json")
        try:
            with open(summary_json_path, 'w') as f:
                json.dump(all_results_summary, f, indent=4)
            logger.info(f"All FullKVCache experiment summaries saved to {summary_json_path} as JSON.")
        except Exception as json_e:
            logger.error(f"Could not save summary as JSON: {json_e}")

    # 处理基线评分（如果启用）
    if args.enable_scoring and args.is_baseline_run and BASELINE_SCORING_AVAILABLE:
        try:
            logger.info("开始处理基线评分...")
            
            # 收集所有实验的评分结果，建立基线
            baseline_scores = []
            
            for result in all_results_summary:
                if isinstance(result, dict) and 'experiment_id' in result:
                    # 查找对应的评分文件
                    experiment_id = result['experiment_id']
                    
                    # 从实验ID中提取数据集名称
                    if 'ds_' in experiment_id:
                        dataset_part = experiment_id.split('ds_')[1].split('_')[0]
                        
                        # 查找评分结果文件
                        for root, dirs, files in os.walk(main_output_dir):
                            for file in files:
                                if file.startswith(f"evaluation_results_") and experiment_id in file:
                                    eval_file_path = os.path.join(root, file)
                                    try:
                                        with open(eval_file_path, 'r', encoding='utf-8') as f:
                                            eval_data = json.load(f)
                                            if eval_data.get("average_score") is not None:
                                                score_result = calculate_relative_score(
                                                    dataset_name=dataset_part,
                                                    raw_score=eval_data["average_score"],
                                                    is_full_kv=True
                                                )
                                                baseline_scores.append(score_result)
                                                logger.info(f"基线分数已记录: {dataset_part} = {eval_data['average_score']:.4f}")
                                    except Exception as e:
                                        logger.warning(f"处理评分文件时出错 {eval_file_path}: {e}")
            
            if baseline_scores:
                # 生成基线报告
                aggregated = aggregate_scores(baseline_scores)
                report = format_score_report(aggregated, "Full KV (基线)")
                
                # 保存基线报告
                baseline_report_path = os.path.join(main_output_dir, "baseline_scoring_report.txt")
                with open(baseline_report_path, 'w', encoding='utf-8') as f:
                    f.write(report)
                
                logger.info(f"基线评分报告已保存到: {baseline_report_path}")
                print(report)
            else:
                logger.warning("未找到有效的评分结果，无法建立基线")
                
        except Exception as baseline_error:
            logger.error(f"处理基线评分时出错: {baseline_error}")
    
    elif args.enable_scoring and not args.is_baseline_run:
        logger.info("评分已启用，但这不是基线运行，跳过基线建立")
    
    logger.info("FullKVCache experiment suite finished.")

if __name__ == "__main__":
    main() 