# FullKVCacheå®éªŒä¸»è„šæœ¬ - å®Œå…¨ä¸ä½¿ç”¨ä»»ä½•ç¼“å­˜ä¼˜åŒ–

import sys
import os

# è®¾ç½®è°ƒè¯•å’Œå†…å­˜ç®¡ç†ç¯å¢ƒå˜é‡
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'  # è®¾å¤‡ç«¯æ–­è¨€æ”¯æŒ
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
current_file_path = os.path.abspath(__file__)
# è·å–é¡¹ç›®ç›®å½•çš„è·¯å¾„
project_dir = os.path.dirname(current_file_path)
# è·å–é¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
project_root_dir = os.path.dirname(project_dir)

# å¦‚æœé¡¹ç›®æ ¹ç›®å½•ä¸åœ¨ sys.path ä¸­ï¼Œåˆ™æ·»åŠ å®ƒ
if project_root_dir not in sys.path:
    sys.path.insert(0, project_root_dir)

"""
FullKVCacheå®éªŒæ‰§è¡Œè„šæœ¬ - ä½¿ç”¨å®Œæ•´KVç¼“å­˜ï¼Œä¸è¿›è¡Œä»»ä½•ä¼˜åŒ–
ä¿®å¤ç‰ˆï¼šè§£å†³CUDAè®¾å¤‡ç«¯æ–­è¨€é”™è¯¯å’Œå†…å­˜ç´¯ç§¯é—®é¢˜
æ”¯æŒæœ¬åœ°JSONLæ•°æ®æ–‡ä»¶
"""
import time
import logging
import argparse
import json
import torch
import random
import numpy as np
import pandas as pd
import gc
from tqdm import tqdm
from datetime import datetime
from transformers import LogitsProcessor, LogitsProcessorList

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from hace_core import config
MODEL_CONFIG = config.MODEL_CONFIG
EXPERIMENT_CONFIG = config.EXPERIMENT_CONFIG
DATASET_CONFIG = config.DATASET_CONFIG
OUTPUT_CONFIG = config.OUTPUT_CONFIG
MONITORING_CONFIG = config.MONITORING_CONFIG

# å¯¼å…¥æ¨¡å— - æ›´æ–°è·¯å¾„ä»¥åŒ¹é…æ–°çš„ç›®å½•ç»“æ„
from hace_core.models.model_loader import (
    load_model_and_tokenizer,
    configure_model_for_kv_cache_length,
    prepare_model_for_baseline
)
from hace_core.data.dataset_loader import load_dataset_split, prepare_samples_for_evaluation, prepare_batch
from hace_core.utils.unified_monitor import UnifiedMonitor

# å¯¼å…¥è¯„åˆ†æ¨¡å—
try:
    longbench_metrics_path = os.path.join(os.path.dirname(__file__), '..', 'cakekv-main', 'cakekv-main', 'experiments', 'LongBench')
    if longbench_metrics_path not in sys.path:
        sys.path.append(longbench_metrics_path)
    
    from metrics import (
        qa_f1_score, rouge_score, classification_score, 
        retrieval_score, count_score, code_sim_score,
        normalize_answer
    )
    SCORING_AVAILABLE = True
    print("[OK] è¯„åˆ†æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"[WARNING] è¯„åˆ†æ¨¡å—åŠ è½½å¤±è´¥: {e}")
    SCORING_AVAILABLE = False

# å¯¼å…¥æ–°çš„åŸºçº¿è¯„åˆ†å·¥å…·
try:
    eval_utils_path = os.path.join(os.path.dirname(__file__), '..')
    if eval_utils_path not in sys.path:
        sys.path.append(eval_utils_path)
    
    from eval_utils import (
        score_dataset,
        calculate_relative_score,
        aggregate_scores,
        format_score_report
    )
    BASELINE_SCORING_AVAILABLE = True
    print("[OK] åŸºçº¿è¯„åˆ†å·¥å…·åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"[WARNING] åŸºçº¿è¯„åˆ†å·¥å…·åŠ è½½å¤±è´¥: {e}")
    BASELINE_SCORING_AVAILABLE = False

# æ•°æ®é›†è¯„åˆ†æ˜ å°„
DATASET_SCORING_MAP = {
    "mmlu": qa_f1_score,
    "narrativeqa": qa_f1_score,
    "qasper": qa_f1_score,
    "multifieldqa_en": qa_f1_score,
    "hotpotqa": qa_f1_score,
    "2wikimqa": qa_f1_score,
    "musique": qa_f1_score,
    "gov_report": rouge_score,
    "qmsum": rouge_score,
    "multi_news": rouge_score,
    "trec": classification_score,
    "triviaqa": qa_f1_score,
    "samsum": rouge_score,
    "passage_retrieval_en": retrieval_score,
    "passage_count": count_score,
    "lcc": code_sim_score,
    "repobench-p": code_sim_score,
}


def comprehensive_cleanup():
    """
    å…¨é¢çš„CUDAå†…å­˜æ¸…ç† - ä¿®å¤ç‰ˆæœ¬
    è§£å†³KV-cacheå®éªŒä¸­çš„å†…å­˜ç´¯ç§¯é—®é¢˜
    """
    try:
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        # æ¸…ç†CUDAç¼“å­˜å’Œä¸Šä¸‹æ–‡
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()
            torch.cuda.synchronize()
            
            # é‡ç½®å†…å­˜ç»Ÿè®¡
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.reset_accumulated_memory_stats()
            
            # æ‰“å°å†…å­˜çŠ¶æ€ç”¨äºè°ƒè¯•
            allocated = torch.cuda.memory_allocated() / 1e9
            reserved = torch.cuda.memory_reserved() / 1e9
            if allocated > 0 or reserved > 0:
                print(f"GPUå†…å­˜çŠ¶æ€ - å·²åˆ†é…: {allocated:.2f}GB, å·²ä¿ç•™: {reserved:.2f}GB")
                
    except Exception as cleanup_error:
        print(f"æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {cleanup_error}")


def validate_kv_cache_inputs(queries, keys, values):
    """
    éªŒè¯KV-cacheè¾“å…¥ä»¥é˜²æ­¢ç´¢å¼•è¶Šç•Œå’Œæ•°å€¼é”™è¯¯
    """
    try:
        # æ£€æŸ¥å¼ é‡ç»´åº¦åŒ¹é…
        if queries.shape[-1] != keys.shape[-1]:
            raise ValueError(f"Q-Kç»´åº¦ä¸åŒ¹é…: {queries.shape[-1]} vs {keys.shape[-1]}")
        
        # æ£€æŸ¥NaNå€¼
        if torch.isnan(queries).any():
            raise ValueError("æŸ¥è¯¢å¼ é‡ä¸­å­˜åœ¨NaNå€¼")
        if torch.isnan(keys).any():
            raise ValueError("é”®å¼ é‡ä¸­å­˜åœ¨NaNå€¼")
        if torch.isnan(values).any():
            raise ValueError("å€¼å¼ é‡ä¸­å­˜åœ¨NaNå€¼")
            
        # æ£€æŸ¥æ— ç©·å€¼
        if torch.isinf(queries).any():
            raise ValueError("æŸ¥è¯¢å¼ é‡ä¸­å­˜åœ¨æ— ç©·å€¼")
        if torch.isinf(keys).any():
            raise ValueError("é”®å¼ é‡ä¸­å­˜åœ¨æ— ç©·å€¼")
        if torch.isinf(values).any():
            raise ValueError("å€¼å¼ é‡ä¸­å­˜åœ¨æ— ç©·å€¼")
            
        return True
    except Exception as e:
        logger.error(f"è¾“å…¥éªŒè¯å¤±è´¥: {e}")
        return False


def monitor_memory():
    """
    å®æ—¶å†…å­˜ç›‘æ§å’Œç¢ç‰‡åŒ–æ£€æµ‹
    """
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        fragmentation = (reserved - allocated) / reserved if reserved > 0 else 0
        
        print(f"GPUå†…å­˜ç›‘æ§ - å·²åˆ†é…: {allocated:.2f}GB, "
              f"å·²ä¿ç•™: {reserved:.2f}GB, "
              f"ç¢ç‰‡åŒ–ç‡: {fragmentation:.2%}")
        
        if fragmentation > 0.3:  # 30%ç¢ç‰‡åŒ–é˜ˆå€¼
            print("âš ï¸ å†…å­˜ç¢ç‰‡åŒ–ä¸¥é‡ï¼Œæ‰§è¡Œæ¸…ç†")
            comprehensive_cleanup()
            
        return allocated, reserved, fragmentation
    return 0, 0, 0


def evaluate_response_quality(prediction, ground_truth, dataset_name, all_classes=None):
    """
    è¯„ä¼°å›ç­”è´¨é‡
    
    Args:
        prediction: æ¨¡å‹ç”Ÿæˆçš„å›ç­”
        ground_truth: æ ‡å‡†ç­”æ¡ˆï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨ï¼‰
        dataset_name: æ•°æ®é›†åç§°
        all_classes: åˆ†ç±»ä»»åŠ¡çš„æ‰€æœ‰ç±»åˆ«
    
    Returns:
        score: è¯„åˆ†ç»“æœ (0-1ä¹‹é—´)
    """
    if not SCORING_AVAILABLE:
        return None
    
    # è·å–è¯„åˆ†å‡½æ•°
    scoring_function = DATASET_SCORING_MAP.get(dataset_name)
    if not scoring_function:
        logger.warning(f"æ•°æ®é›† {dataset_name} æš‚ä¸æ”¯æŒè‡ªåŠ¨è¯„åˆ†")
        return None
    
    try:
        # å¤„ç†å¤šä¸ªæ ‡å‡†ç­”æ¡ˆçš„æƒ…å†µ
        if isinstance(ground_truth, list):
            scores = []
            for gt in ground_truth:
                score = scoring_function(prediction, gt, all_classes=all_classes)
                scores.append(score)
            return max(scores)  # å–æœ€é«˜åˆ†
        else:
            return scoring_function(prediction, ground_truth, all_classes=all_classes)
    except Exception as e:
        logger.error(f"è¯„åˆ†æ—¶å‡ºé”™: {e}")
        return None


def extract_ground_truth_from_sample(sample, dataset_source):
    """ä¿®å¤ç‰ˆï¼šç›´æ¥ä½¿ç”¨æ­£ç¡®çš„ç­”æ¡ˆå­—æ®µ"""
    print(f"[DEBUG] æå–ç­”æ¡ˆ - æ ·æœ¬é”®: {list(sample.keys())}")
    
    # ä¼˜å…ˆä½¿ç”¨æˆ‘ä»¬æ‰‹åŠ¨è®¾ç½®çš„referenceå­—æ®µ
    if 'reference' in sample and sample['reference']:
        reference = sample['reference']
        print(f"[DEBUG] ä½¿ç”¨referenceå­—æ®µ: {reference}")
        if isinstance(reference, list):
            return [str(item) for item in reference]
        else:
            return [str(reference)]
    
    # å›é€€åˆ°åŸå§‹æ ·æœ¬
    if 'original_sample' in sample:
        original = sample['original_sample']
        if 'answers' in original:
            answers = original['answers']
            print(f"[DEBUG] ä½¿ç”¨åŸå§‹æ ·æœ¬answers: {answers}")
            if isinstance(answers, list):
                return [str(item) for item in answers]
            else:
                return [str(answers)]
    
    # å…¶ä»–å­—æ®µæ£€æŸ¥...
    answer_fields = ['answers', 'answer', 'output', 'gold', 'target', 'label']
    for field in answer_fields:
        if field in sample and sample[field]:
            value = sample[field]
            print(f"[DEBUG] æ‰¾åˆ°å­—æ®µ '{field}': {value}")
            if isinstance(value, list):
                return [str(item) for item in value]
            else:
                return [str(value)]
    
    print(f"[DEBUG] æœªæ‰¾åˆ°ç­”æ¡ˆï¼Œå®Œæ•´æ ·æœ¬: {sample}")
    return ["Unknown"]


def load_local_jsonl(dataset_name, data_dir="../../data"):
    """
    ä»æœ¬åœ°JSONLæ–‡ä»¶åŠ è½½æ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        dataset: æ•°æ®åˆ—è¡¨
    """
    file_path = os.path.join(data_dir, f"{dataset_name}.jsonl")
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"âŒ æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:  # è·³è¿‡ç©ºè¡Œ
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    logger.warning(f"è·³è¿‡æ— æ•ˆçš„JSONè¡Œ: {line[:100]}... é”™è¯¯: {e}")
    
    logger.info(f"âœ… ä»æœ¬åœ°åŠ è½½ {dataset_name}ï¼Œå…± {len(data)} æ¡æ ·æœ¬")
    return data


def load_dataset_with_fallback(dataset_name, dataset_options, split="validation"):
    """
    åŠ è½½æ•°æ®é›†ï¼Œä¼˜å…ˆä½¿ç”¨Hugging Faceï¼Œå¤±è´¥æ—¶å›é€€åˆ°æœ¬åœ°JSONLæ–‡ä»¶
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        dataset_options: æ•°æ®é›†é…ç½®é€‰é¡¹
        split: æ•°æ®åˆ†å‰²åç§°
        
    Returns:
        dataset: åŠ è½½çš„æ•°æ®é›†
        source: æ•°æ®æº ("huggingface" æˆ– "local")
    """
    # é¦–å…ˆå°è¯•ä»æœ¬åœ°JSONLæ–‡ä»¶åŠ è½½ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼Œç¡®ä¿ä½¿ç”¨å¸¦ç­”æ¡ˆçš„éªŒè¯é›†ï¼‰
    try:
        logger.info(f"å°è¯•ä»æœ¬åœ°JSONLæ–‡ä»¶åŠ è½½æ•°æ®é›†: {dataset_name}")
        local_data = load_local_jsonl(dataset_name)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°æ®é›†å¯¹è±¡ï¼Œæ¨¡æ‹Ÿdatasetsåº“çš„æ ¼å¼
        class SimpleDataset:
            def __init__(self, data):
                self.data = data
                
            def __len__(self):
                return len(self.data)
                
            def __getitem__(self, idx):
                return self.data[idx]
                
            def __iter__(self):
                return iter(self.data)
        
        dataset = SimpleDataset(local_data)
        logger.info(f"âœ… æˆåŠŸä»æœ¬åœ°JSONLæ–‡ä»¶åŠ è½½ {dataset_name} (æ¥æº: local)")
        return dataset, "local"
    except Exception as local_error:
        logger.warning(f"âš ï¸ æ— æ³•ä»æœ¬åœ°åŠ è½½ {dataset_name}: {local_error}")
        logger.info(f"å›é€€åˆ°ä»Hugging FaceåŠ è½½æ•°æ®é›†: {dataset_name}")
        try:
            # å›é€€åˆ°Hugging FaceåŠ è½½
            dataset = load_dataset_split(dataset_options, split=split)
            logger.info(f"âœ… æˆåŠŸä»Hugging FaceåŠ è½½ {dataset_name} (æ¥æº: huggingface)")
            return dataset, "huggingface"
        except Exception as hf_error:
            logger.error(f"âŒ æ— æ³•ä»Hugging FaceåŠ è½½ {dataset_name}: {hf_error}")
            raise Exception(f"æ— æ³•ä»ä»»ä½•æ¥æºåŠ è½½æ•°æ®é›† {dataset_name}ã€‚æœ¬åœ°é”™è¯¯: {local_error}. Hugging Faceé”™è¯¯: {hf_error}")


# è®¾ç½®æ—¥å¿—
def setup_logging(log_file=None, level=logging.INFO):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    log_dir = None
    if log_file:
        log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)

    handlers = []
    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    handlers.append(logging.StreamHandler())

    logging.basicConfig(
        level=level,
        format=log_format,
        handlers=handlers,
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®æ—¥å¿—
    )
    # å‡å°‘ä¸€äº›åº“çš„æ—¥å¿—è¾“å‡º
    logging.getLogger("transformers").setLevel(logging.WARNING)
    logging.getLogger("datasets").setLevel(logging.WARNING)

    return logging.getLogger(__name__)

logger = logging.getLogger(__name__)

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    logger.info(f"Random seed set to {seed}")


def run_fullkvcache_experiment(model_config, dataset_name, dataset_options,
                              kv_cache_length, batch_size, max_new_tokens,
                              output_dir, repeat_index=0):
    """
    è¿è¡Œå•æ¬¡FullKVCacheå®éªŒ - ä¿®å¤ç‰ˆ
    æ·»åŠ å…¨é¢çš„å†…å­˜ç®¡ç†å’Œé”™è¯¯å¤„ç†

    Args:
        model_config: æ¨¡å‹é…ç½®
        dataset_name: æ•°æ®é›†åç§°
        dataset_options: æ•°æ®é›†é…ç½®é€‰é¡¹ 
        kv_cache_length: KVç¼“å­˜é•¿åº¦
        batch_size: æ‰¹å¤„ç†å¤§å°
        max_new_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
        output_dir: è¾“å‡ºç›®å½•
        repeat_index: é‡å¤å®éªŒçš„ç´¢å¼•

    Returns:
        metrics: æ€§èƒ½æŒ‡æ ‡
    """
    run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    experiment_id = f"fullkvcache_{dataset_name}_kv{kv_cache_length}_bs{batch_size}_rep{repeat_index}_{run_timestamp}"
    logger.info(f"Starting FullKVCache experiment: {experiment_id}")

    # å®éªŒå‰æ¸…ç†
    comprehensive_cleanup()
    logger.info("å®éªŒå‰å†…å­˜æ¸…ç†å®Œæˆ")

    # åˆå§‹åŒ–ç»Ÿä¸€ç›‘æ§å™¨
    monitor = UnifiedMonitor(experiment_id=experiment_id)
    monitor.record_config({
        "model_name": model_config["model_name_or_path"],
        "precision": model_config["precision"],
        "batch_size": batch_size,
        "kv_cache_length": kv_cache_length,
        "max_new_tokens": max_new_tokens,
        "use_fullkvcache": True,
        "dataset": dataset_name,
        "repetition": repeat_index
    })

    # åˆå§‹åŒ–å˜é‡
    model = None
    tokenizer = None
    inputs = None
    outputs = None
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        logger.info("Loading model and tokenizer...")
        monitor_memory()  # ç›‘æ§åŠ è½½å‰çš„å†…å­˜çŠ¶æ€
        
        model, tokenizer = load_model_and_tokenizer(model_config)
        logger.info(f"æ¨¡å‹åŠ è½½åGPUå†…å­˜: {torch.cuda.memory_allocated()/1e9:.2f}GB")

        # é…ç½®æ¨¡å‹çš„KVç¼“å­˜é•¿åº¦ï¼Œä½†ä¸è¿›è¡Œä»»ä½•ä¼˜åŒ–
        model = configure_model_for_kv_cache_length(model, kv_cache_length)

        # å‡†å¤‡åŸºçº¿æ¨¡å‹ï¼ˆå®Œæ•´KVç¼“å­˜ï¼Œæ— ä¼˜åŒ–ï¼‰
        model = prepare_model_for_baseline(model)
        
        # ç¡®ä¿ä½¿ç”¨å®Œæ•´ç¼“å­˜
        model.config.use_cache = True
        
        # RTX 4090ç‰¹å®šä¼˜åŒ–
        if torch.cuda.is_available():
            torch.backends.cuda.max_split_size_mb = 128
            torch.backends.cudnn.benchmark = True

        # åŠ è½½æ•°æ®é›†ï¼Œä½¿ç”¨æ–°çš„å›é€€æœºåˆ¶
        logger.info(f"Loading dataset {dataset_name}...")
        dataset, dataset_source = load_dataset_with_fallback(dataset_name, dataset_options, split="test")

        # å‡†å¤‡è¯„ä¼°æ ·æœ¬ï¼ˆä¸ºäº†è¯„åˆ†ï¼Œä½¿ç”¨è¾ƒå°çš„æ ·æœ¬æ•°ï¼‰
        num_eval_samples = EXPERIMENT_CONFIG.get("dataset_subset_size", {}).get(dataset_name)
        if num_eval_samples is None:
            num_eval_samples = min(20, len(dataset))  # å‡å°‘åˆ°20ä¸ªæ ·æœ¬ä»¥ä¾¿è¯„åˆ†
        
        actual_num_samples_to_prepare = min(batch_size, num_eval_samples)
        if actual_num_samples_to_prepare == 0:
            error_msg = f"æ²¡æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¿›è¡Œå®éªŒ (éœ€è¦ {batch_size}, å¯ç”¨ {num_eval_samples})ã€‚"
            logger.error(error_msg)
            monitor.mark_failure(error_msg)
            return monitor.get_comprehensive_metrics()

        # ä¸´æ—¶è°ƒè¯•ï¼šæ£€æŸ¥é¢„å¤„ç†å‰åçš„æ ·æœ¬
        print(f"[DEBUG] åŸå§‹æ•°æ®é›†ç¬¬ä¸€ä¸ªæ ·æœ¬: {dataset[0] if hasattr(dataset, '__getitem__') else 'No direct access'}")
        
        # ğŸ”§ ç»•è¿‡æœ‰é—®é¢˜çš„é¢„å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®
        print("ğŸ”§ ç»•è¿‡æœ‰é—®é¢˜çš„é¢„å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®")
        samples = []
        for i in range(min(actual_num_samples_to_prepare, len(dataset))):
            original_sample = dataset[i]
            # æ‰‹åŠ¨åˆ›å»ºæ­£ç¡®çš„æ ·æœ¬æ ¼å¼
            processed_sample = {
                'prompt': original_sample['input'],
                'reference': original_sample['answers'] if isinstance(original_sample['answers'], list) else [original_sample['answers']],
                'context': original_sample.get('context', ''),
                'original_sample': original_sample  # ä¿ç•™åŸå§‹æ ·æœ¬ç”¨äºè°ƒè¯•
            }
            samples.append(processed_sample)
            print(f"[DEBUG] æ‰‹åŠ¨å¤„ç†æ ·æœ¬ {i+1}: prompt={processed_sample['prompt'][:50]}...")
            print(f"[DEBUG] æ‰‹åŠ¨å¤„ç†æ ·æœ¬ {i+1}: reference={processed_sample['reference']}")
        
        print(f"[DEBUG] é¢„å¤„ç†åæ ·æœ¬: {samples[0] if samples else 'No samples'}")

        # å‡†å¤‡æ‰¹å¤„ç†
        effective_max_length = min(kv_cache_length, model.config.max_position_embeddings)
        logger.info(f"Preparing batch with size {batch_size}, max_length {effective_max_length}...")
        
        batch = prepare_batch(
            samples,
            tokenizer,
            batch_size=actual_num_samples_to_prepare,
            max_length=effective_max_length
        )

        # å°†æ‰¹å¤„ç†æ•°æ®ç§»è‡³è®¾å¤‡
        inputs = {
            "input_ids": batch["input_ids"].to(model.device),
            "attention_mask": batch["attention_mask"].to(model.device)
        }
        if "token_type_ids" in batch and batch["token_type_ids"] is not None:
             inputs["token_type_ids"] = batch["token_type_ids"].to(model.device)
        
        # éªŒè¯è¾“å…¥æ•°æ®
        for key, tensor in inputs.items():
            if torch.isnan(tensor).any():
                raise ValueError(f"è¾“å…¥æ•°æ® {key} åŒ…å«NaNå€¼")
            if torch.isinf(tensor).any():
                raise ValueError(f"è¾“å…¥æ•°æ® {key} åŒ…å«æ— ç©·å€¼")
            if tensor.max() >= model.config.vocab_size and key == "input_ids":
                raise ValueError(f"è¾“å…¥token IDè¶…å‡ºè¯æ±‡è¡¨èŒƒå›´: {tensor.max()} >= {model.config.vocab_size}")
        
        logger.info("è¾“å…¥æ•°æ®éªŒè¯é€šè¿‡")

        # é¢„çƒ­ï¼ˆå¯é€‰ï¼‰
        logger.info("Warming up FullKVCache model...")
        with torch.no_grad():
            _ = model.generate(
                **inputs,
                max_new_tokens=min(5, max_new_tokens),
                do_sample=False,
                use_cache=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        time.sleep(0.5)

        # å¯åŠ¨ç»Ÿä¸€ç›‘æ§
        if EXPERIMENT_CONFIG.get("enable_monitoring", True):
            monitor.start_monitoring()

        # å¼€å§‹æ€§èƒ½æµ‹é‡
        logger.info("Starting FullKVCache performance measurement...")
        monitor.start_generation()

        # å®šä¹‰è‡ªå®šä¹‰ LogitsProcessor æ¥è®°å½•ä»¤ç‰Œç”Ÿæˆæ—¶é—´
        class TokenTimeLogitsProcessor(LogitsProcessor):
            def __init__(self, monitor):
                self.monitor = monitor
                self.first_token_recorded = False

            def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):
                if not self.first_token_recorded:
                    self.monitor.record_first_token()
                    self.first_token_recorded = True
                else:
                    self.monitor.record_token()
                return scores

        token_time_processor = TokenTimeLogitsProcessor(monitor)
        logits_processor_list = LogitsProcessorList([token_time_processor])

        # ç”Ÿæˆé…ç½®
        generate_kwargs = DATASET_CONFIG.get("generate_config", {}).copy()
        generate_kwargs.update({
            "max_new_tokens": max_new_tokens,
            "logits_processor": logits_processor_list,
            "pad_token_id": tokenizer.pad_token_id,
            "eos_token_id": tokenizer.eos_token_id,
            "use_cache": True  # æ˜ç¡®å¯ç”¨å®Œæ•´ç¼“å­˜
        })

        # ç”Ÿæˆæ–‡æœ¬
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **generate_kwargs
            )

        # ç»“æŸæ€§èƒ½æµ‹é‡
        monitor.end_generation()

        # è‡ªåŠ¨è¯„åˆ†å¤„ç†
        evaluation_results = []
        total_score = 0.0
        scored_samples = 0
        
        if SCORING_AVAILABLE:
            logger.info("å¼€å§‹è‡ªåŠ¨è¯„åˆ†...")
            try:
                # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                input_length = inputs["input_ids"].shape[1]
                generated_tokens = outputs[:, input_length:]
                
                for i in range(generated_tokens.shape[0]):
                    try:
                        generated_text = tokenizer.decode(generated_tokens[i], skip_special_tokens=True)
                        
                        # è·å–å¯¹åº”çš„åŸå§‹æ ·æœ¬å’Œæ ‡å‡†ç­”æ¡ˆ
                        if i < len(samples):
                            sample = samples[i]
                            ground_truth = extract_ground_truth_from_sample(sample, dataset_source)
                            
                            # è®¡ç®—åˆ†æ•°
                            score = evaluate_response_quality(generated_text, ground_truth, dataset_name)
                            
                            if score is not None:
                                total_score += score
                                scored_samples += 1
                                evaluation_results.append({
                                    "sample_id": i,
                                    "prediction": generated_text[:500],  # é™åˆ¶é•¿åº¦
                                    "ground_truth": str(ground_truth)[:200] if ground_truth else "Unknown",
                                    "score": score
                                })
                                logger.info(f"æ ·æœ¬ {i+1} è¯„åˆ†: {score:.3f}")
                        
                    except Exception as e:
                        logger.warning(f"è¯„åˆ†æ ·æœ¬ {i+1} æ—¶å‡ºé”™: {e}")
                
                # è®¡ç®—å¹³å‡åˆ†æ•°
                # éªŒè¯æ˜¯å¦æœ‰æœ‰æ•ˆçš„ground truth
                invalid_gt_count = sum(1 for result in evaluation_results if result.get('ground_truth') == "['Unknown']")
                if invalid_gt_count > 0:
                    logger.warning(f"âš ï¸ å‘ç° {invalid_gt_count} ä¸ªæ ·æœ¬çš„ground truthä¸ºUnknownï¼Œè¯„åˆ†å¯èƒ½æ— æ•ˆ")
                    logger.warning("è¯·æ£€æŸ¥æ•°æ®é›†æ ¼å¼å’Œç­”æ¡ˆæå–é€»è¾‘")
                
                if scored_samples > 0:
                    average_score = total_score / scored_samples
                    logger.info(f"âœ… è¯„åˆ†å®Œæˆ! å¹³å‡åˆ†æ•°: {average_score:.3f} ({scored_samples}/{len(samples)} ä¸ªæ ·æœ¬)")
                    
                    # å°†è¯„åˆ†ç»“æœæ·»åŠ åˆ°ç›‘æ§æŒ‡æ ‡ä¸­
                    monitor.performance_metrics["evaluation"] = {
                        "average_score": average_score,
                        "total_score": total_score,
                        "scored_samples": scored_samples,
                        "total_samples": len(samples),
                        "scoring_coverage": scored_samples / len(samples) if len(samples) > 0 else 0,
                        "individual_results": evaluation_results
                    }
                else:
                    logger.warning("âš ï¸ æ²¡æœ‰æˆåŠŸè¯„åˆ†çš„æ ·æœ¬")
                    
            except Exception as e:
                logger.error(f"è¯„åˆ†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        else:
            logger.info("è¯„åˆ†æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡è‡ªåŠ¨è¯„åˆ†")

        # åœæ­¢ç›‘æ§å¹¶æ”¶é›†æŒ‡æ ‡
        if EXPERIMENT_CONFIG.get("enable_monitoring", True):
            monitor.stop_monitoring()

        # è®¡ç®—å’Œä¿å­˜æŒ‡æ ‡
        final_metrics = monitor.get_comprehensive_metrics()
        monitor.save_metrics(output_dir, filename=f"fullkvcache_metrics_{experiment_id}.json")
        
        # ä¿å­˜è¯„åˆ†ç»“æœ
        if evaluation_results:
            eval_file = os.path.join(output_dir, f"evaluation_results_{experiment_id}.json")
            try:
                with open(eval_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "experiment_id": experiment_id,
                        "dataset": dataset_name,
                        "average_score": average_score if scored_samples > 0 else 0,
                        "results": evaluation_results
                    }, f, indent=2, ensure_ascii=False)
                logger.info(f"è¯„åˆ†ç»“æœå·²ä¿å­˜åˆ°: {eval_file}")
            except Exception as e:
                logger.error(f"ä¿å­˜è¯„åˆ†ç»“æœæ—¶å‡ºé”™: {e}")
        
        logger.info(f"FullKVCache Experiment {experiment_id} completed. Metrics: {final_metrics}")
        return final_metrics

    except RuntimeError as e:
        error_msg = str(e)
        if "device-side assert" in error_msg:
            logger.error(f"æ£€æµ‹åˆ°CUDAè®¾å¤‡ç«¯æ–­è¨€é”™è¯¯: {error_msg}")
            logger.info("åˆ‡æ¢åˆ°CPUè¿›è¡Œè°ƒè¯•...")
            try:
                # å°†æ¨¡å‹åˆ‡æ¢åˆ°CPUè·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
                if model is not None:
                    model_cpu = model.cpu()
                    if inputs is not None:
                        inputs_cpu = {k: v.cpu() for k, v in inputs.items()}
                        # å°è¯•åœ¨CPUä¸Šè¿è¡Œä»¥è·å–çœŸå®é”™è¯¯
                        with torch.no_grad():
                            _ = model_cpu.generate(**inputs_cpu, max_new_tokens=5)
                    del model_cpu
            except Exception as cpu_error:
                logger.error(f"CPUè°ƒè¯•æ˜¾ç¤ºçœŸå®é”™è¯¯: {cpu_error}")
        
        logger.error(f"è¿è¡Œæ—¶é”™è¯¯ - å®éªŒ {experiment_id}: {error_msg}", exc_info=True)
        monitor.mark_failure(error_msg)
        return monitor.get_comprehensive_metrics()
    except Exception as e:
        logger.error(f"Error during FullKVCache experiment {experiment_id}: {e}", exc_info=True)
        monitor.mark_failure(str(e))
        return monitor.get_comprehensive_metrics()
    finally:
        # å…¨é¢æ¸…ç†æ¨¡å‹å’ŒGPUå†…å­˜
        try:
            logger.info(f"å¼€å§‹æ¸…ç†å®éªŒ {experiment_id} çš„èµ„æº...")
            
            # åˆ é™¤æ‰€æœ‰å¤§å¯¹è±¡
            if model is not None:
                del model
            if tokenizer is not None:
                del tokenizer
            if inputs is not None:
                del inputs
            if outputs is not None:
                del outputs
            
            # æ‰§è¡Œå…¨é¢æ¸…ç†
            comprehensive_cleanup()
            
            logger.info(f"å®éªŒ {experiment_id} èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as cleanup_error:
            logger.warning(f"æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {cleanup_error}")


def main():
    parser = argparse.ArgumentParser(description="Run FullKVCache Experiments")
    parser.add_argument("--model_name", type=str, default=EXPERIMENT_CONFIG["model_name_or_path"], help="Name or path of the model to use.")
    parser.add_argument("--datasets", type=str, default=",".join(EXPERIMENT_CONFIG["datasets"]), help="Comma-separated list of datasets to use.")
    parser.add_argument("--kv_cache_lengths", type=str, default=",".join(map(str, EXPERIMENT_CONFIG["kv_cache_lengths"])), help="Comma-separated list of KV cache lengths.")
    parser.add_argument("--batch_sizes", type=str, default=",".join(map(str, EXPERIMENT_CONFIG["batch_sizes"])), help="Comma-separated list of batch sizes.")
    parser.add_argument("--max_new_tokens", type=int, default=EXPERIMENT_CONFIG["max_new_tokens"], help="Maximum number of new tokens to generate.")
    parser.add_argument("--repetitions", type=int, default=EXPERIMENT_CONFIG["repetitions"], help="Number of repetitions for each experiment configuration.")
    parser.add_argument("--output_dir", type=str, default=os.path.join(EXPERIMENT_CONFIG["output_base_dir"], "fullkvcache_experiments"), help="Directory to save experiment results.")
    parser.add_argument("--log_level", type=str, default="INFO", help="Logging level (DEBUG, INFO, WARNING, ERROR)")
    parser.add_argument("--seed", type=int, default=config.EXPERIMENT_CONFIG.get("random_seed", 42), help="Random seed for reproducibility.")
    parser.add_argument("--run_name", type=str, default=f"fullkvcache_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}", help="A specific name for this run/sweep of experiments.")
    parser.add_argument("--enable_scoring", action="store_true", help="Enable scoring evaluation")
    parser.add_argument("--is_baseline_run", action="store_true", help="Mark this as a baseline run for establishing Full KV baseline scores")

    args = parser.parse_args()

    # åˆ›å»ºæœ¬æ¬¡è¿è¡Œçš„æ€»è¾“å‡ºç›®å½•
    main_output_dir = os.path.join(args.output_dir, args.run_name)
    os.makedirs(main_output_dir, exist_ok=True)

    # è®¾ç½®æ—¥å¿—
    log_file_path = os.path.join(main_output_dir, "fullkvcache_experiment_log.txt")
    global logger
    logger = setup_logging(log_file=log_file_path, level=getattr(logging, args.log_level.upper(), logging.INFO))

    logger.info(f"Starting FullKVCache experiment suite with run name: {args.run_name}")
    logger.info(f"Arguments: {args}")
    logger.info(f"Global EXPERIMENT_CONFIG being used: {EXPERIMENT_CONFIG}")

    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)

    # è§£æå‚æ•°åˆ—è¡¨
    datasets_list = [d.strip() for d in args.datasets.split(',') if d.strip()]
    kv_lengths_list = [int(kv.strip()) for kv in args.kv_cache_lengths.split(',') if kv.strip()]
    batch_sizes_list = [int(bs.strip()) for bs in args.batch_sizes.split(',') if bs.strip()]

    all_results_summary = []
    total_experiments = len(datasets_list) * len(kv_lengths_list) * len(batch_sizes_list) * args.repetitions
    logger.info(f"Total number of FullKVCache experiment configurations to run: {total_experiments}")
    pbar = tqdm(total=total_experiments, desc="Running FullKVCache Experiments")

    current_model_config = {
        "model_name_or_path": args.model_name,
        "precision": EXPERIMENT_CONFIG["precision"]
    }

    for rep in range(args.repetitions):
        for dataset_name in datasets_list:
            # è·å–æ•°æ®é›†é…ç½®
            dataset_options = DATASET_CONFIG.get("available_datasets", {}).get(dataset_name)
            if not dataset_options:
                logger.error(f"Dataset configuration for '{dataset_name}' not found in DATASET_CONFIG. Skipping...")
                pbar.update(len(kv_lengths_list) * len(batch_sizes_list))
                continue
            
            for kv_len in kv_lengths_list:
                for bs in batch_sizes_list:
                    logger.info(f"Running FullKVCache: Rep {rep+1}/{args.repetitions}, Dataset: {dataset_name}, KV_Len: {kv_len}, Batch: {bs}")
                    
                    # å®éªŒé—´å†…å­˜æ¸…ç†
                    comprehensive_cleanup()
                    monitor_memory()
                    
                    # ä¸ºå½“å‰å®éªŒåˆ›å»ºç‰¹å®šçš„è¾“å‡ºå­ç›®å½•
                    exp_label = f"ds_{dataset_name}_kv{kv_len}_bs{bs}_rep{rep}"
                    current_exp_output_dir = os.path.join(main_output_dir, exp_label)
                    os.makedirs(current_exp_output_dir, exist_ok=True)

                    try:
                        experiment_metrics = run_fullkvcache_experiment(
                            model_config=current_model_config,
                            dataset_name=dataset_name,
                            dataset_options=dataset_options,
                            kv_cache_length=kv_len,
                            batch_size=bs,
                            max_new_tokens=args.max_new_tokens,
                            output_dir=current_exp_output_dir,
                            repeat_index=rep
                        )
                        all_results_summary.append(experiment_metrics)
                        logger.info(f"âœ“ å®éªŒæˆåŠŸå®Œæˆ: {exp_label}")
                    except Exception as exp_error:
                        logger.error(f"âœ— å®éªŒå¤±è´¥: {exp_label}, é”™è¯¯: {exp_error}")
                        # è®°å½•å¤±è´¥çš„å®éªŒ
                        failed_metrics = {
                            "experiment_id": exp_label,
                            "error": str(exp_error),
                            "status": "failed"
                        }
                        all_results_summary.append(failed_metrics)
                    finally:
                        # ç¡®ä¿æ¯ä¸ªå®éªŒåéƒ½æ¸…ç†
                        comprehensive_cleanup()
                    
                    pbar.update(1)
    pbar.close()

    # ä¿å­˜æ‰€æœ‰å®éªŒç»“æœçš„æ±‡æ€»
    summary_file_path = os.path.join(main_output_dir, "all_fullkvcache_experiments_summary.csv")
    if all_results_summary and isinstance(all_results_summary[0], dict):
        summary_df = pd.DataFrame(all_results_summary)
        summary_df.to_csv(summary_file_path, index=False)
        logger.info(f"All FullKVCache experiment summaries saved to {summary_file_path}")
    elif all_results_summary:
        logger.warning(f"Result summary items are not all dicts, cannot easily save to CSV. First item: {all_results_summary[0]}")
        summary_json_path = os.path.join(main_output_dir, "all_fullkvcache_experiments_summary.json")
        try:
            with open(summary_json_path, 'w') as f:
                json.dump(all_results_summary, f, indent=4)
            logger.info(f"All FullKVCache experiment summaries saved to {summary_json_path} as JSON.")
        except Exception as json_e:
            logger.error(f"Could not save summary as JSON: {json_e}")

    # å¤„ç†åŸºçº¿è¯„åˆ†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.enable_scoring and args.is_baseline_run and BASELINE_SCORING_AVAILABLE:
        try:
            logger.info("å¼€å§‹å¤„ç†åŸºçº¿è¯„åˆ†...")
            
            # æ”¶é›†æ‰€æœ‰å®éªŒçš„è¯„åˆ†ç»“æœï¼Œå»ºç«‹åŸºçº¿
            baseline_scores = []
            
            for result in all_results_summary:
                if isinstance(result, dict) and 'experiment_id' in result:
                    # æŸ¥æ‰¾å¯¹åº”çš„è¯„åˆ†æ–‡ä»¶
                    experiment_id = result['experiment_id']
                    
                    # ä»å®éªŒIDä¸­æå–æ•°æ®é›†åç§°
                    if 'ds_' in experiment_id:
                        dataset_part = experiment_id.split('ds_')[1].split('_')[0]
                        
                        # æŸ¥æ‰¾è¯„åˆ†ç»“æœæ–‡ä»¶
                        for root, dirs, files in os.walk(main_output_dir):
                            for file in files:
                                if file.startswith(f"evaluation_results_") and experiment_id in file:
                                    eval_file_path = os.path.join(root, file)
                                    try:
                                        with open(eval_file_path, 'r', encoding='utf-8') as f:
                                            eval_data = json.load(f)
                                            if eval_data.get("average_score") is not None:
                                                score_result = calculate_relative_score(
                                                    dataset_name=dataset_part,
                                                    raw_score=eval_data["average_score"],
                                                    is_full_kv=True
                                                )
                                                baseline_scores.append(score_result)
                                                logger.info(f"åŸºçº¿åˆ†æ•°å·²è®°å½•: {dataset_part} = {eval_data['average_score']:.4f}")
                                    except Exception as e:
                                        logger.warning(f"å¤„ç†è¯„åˆ†æ–‡ä»¶æ—¶å‡ºé”™ {eval_file_path}: {e}")
            
            if baseline_scores:
                # ç”ŸæˆåŸºçº¿æŠ¥å‘Š
                aggregated = aggregate_scores(baseline_scores)
                report = format_score_report(aggregated, "Full KV (åŸºçº¿)")
                
                # ä¿å­˜åŸºçº¿æŠ¥å‘Š
                baseline_report_path = os.path.join(main_output_dir, "baseline_scoring_report.txt")
                with open(baseline_report_path, 'w', encoding='utf-8') as f:
                    f.write(report)
                
                logger.info(f"åŸºçº¿è¯„åˆ†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {baseline_report_path}")
                print(report)
            else:
                logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¯„åˆ†ç»“æœï¼Œæ— æ³•å»ºç«‹åŸºçº¿")
                
        except Exception as baseline_error:
            logger.error(f"å¤„ç†åŸºçº¿è¯„åˆ†æ—¶å‡ºé”™: {baseline_error}")
    
    elif args.enable_scoring and not args.is_baseline_run:
        logger.info("è¯„åˆ†å·²å¯ç”¨ï¼Œä½†è¿™ä¸æ˜¯åŸºçº¿è¿è¡Œï¼Œè·³è¿‡åŸºçº¿å»ºç«‹")
    
    logger.info("FullKVCache experiment suite finished.")

if __name__ == "__main__":
    main() 