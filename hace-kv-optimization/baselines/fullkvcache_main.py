# FullKVCacheå®éªŒä¸»è„šæœ¬ - å®Œå…¨ä¸ä½¿ç”¨ä»»ä½•ç¼“å­˜ä¼˜åŒ–

import sys
import os

# è®¾ç½®è°ƒè¯•å’Œå†…å­˜ç®¡ç†ç¯å¢ƒå˜é‡
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'  # è®¾å¤‡ç«¯æ–­è¨€æ”¯æŒ
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
current_file_path = os.path.abspath(__file__)
# è·å–é¡¹ç›®ç›®å½•çš„è·¯å¾„
project_dir = os.path.dirname(current_file_path)
# è·å–é¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
project_root_dir = os.path.dirname(project_dir)

# å¦‚æœé¡¹ç›®æ ¹ç›®å½•ä¸åœ¨ sys.path ä¸­ï¼Œåˆ™æ·»åŠ å®ƒ
if project_root_dir not in sys.path:
    sys.path.insert(0, project_root_dir)

"""
FullKVCacheå®éªŒæ‰§è¡Œè„šæœ¬ - ä½¿ç”¨å®Œæ•´KVç¼“å­˜ï¼Œä¸è¿›è¡Œä»»ä½•ä¼˜åŒ–
ä¿®å¤ç‰ˆï¼šè§£å†³CUDAè®¾å¤‡ç«¯æ–­è¨€é”™è¯¯å’Œå†…å­˜ç´¯ç§¯é—®é¢˜
æ”¯æŒæœ¬åœ°JSONLæ•°æ®æ–‡ä»¶
"""
import time
import logging
import argparse
import json
import torch
import random
import numpy as np
import pandas as pd
import gc
from tqdm import tqdm
from datetime import datetime
from transformers import LogitsProcessor, LogitsProcessorList

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from hace_core import config

MODEL_CONFIG = config.MODEL_CONFIG
EXPERIMENT_CONFIG = config.EXPERIMENT_CONFIG
DATASET_CONFIG = config.DATASET_CONFIG
OUTPUT_CONFIG = config.OUTPUT_CONFIG
MONITORING_CONFIG = config.MONITORING_CONFIG

# å¯¼å…¥æ¨¡å— - æ›´æ–°è·¯å¾„ä»¥åŒ¹é…æ–°çš„ç›®å½•ç»“æ„
from hace_core.models.model_loader import (
    load_model_and_tokenizer,
    configure_model_for_kv_cache_length,
    prepare_model_for_baseline
)
from hace_core.data.dataset_loader import load_dataset_split, prepare_samples_for_evaluation, prepare_batch
from hace_core.utils.unified_monitor import UnifiedMonitor


# é‡å†™é…ç½®ä»¥ä½¿ç”¨ç›¸å¯¹è·¯å¾„
def override_config_paths():
    """é‡å†™é…ç½®ä¸ºç›¸å¯¹è·¯å¾„"""
    import os
    from pathlib import Path

    # è·å–å½“å‰å·¥ä½œç›®å½•
    current_dir = Path.cwd()

    # é‡å†™è¾“å‡ºç›®å½•é…ç½®
    if hasattr(config, 'EXPERIMENT_CONFIG'):
        config.EXPERIMENT_CONFIG["output_base_dir"] = str(current_dir / "experiments")
    if hasattr(config, 'OUTPUT_CONFIG'):
        config.OUTPUT_CONFIG["base_dir"] = str(current_dir / "results")

    print(f"âœ… é…ç½®å·²é‡å†™ä¸ºç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºç›®å½•: {current_dir}")


# è°ƒç”¨é…ç½®é‡å†™
override_config_paths()

# å¯¼å…¥è¯„åˆ†æ¨¡å—
try:
    longbench_metrics_path = os.path.join(os.path.dirname(__file__), '..', 'cakekv-main', 'cakekv-main', 'experiments',
                                          'LongBench')
    if longbench_metrics_path not in sys.path:
        sys.path.append(longbench_metrics_path)

    from metrics import (
        qa_f1_score, rouge_score, classification_score,
        retrieval_score, count_score, code_sim_score,
        normalize_answer
    )

    SCORING_AVAILABLE = True
    print("[OK] è¯„åˆ†æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"[WARNING] è¯„åˆ†æ¨¡å—åŠ è½½å¤±è´¥: {e}")
    SCORING_AVAILABLE = False


    # å®šä¹‰å ä½ç¬¦å‡½æ•°ï¼Œé¿å…NameError
    def qa_f1_score(*args, **kwargs):
        return None


    def rouge_score(*args, **kwargs):
        return None


    def classification_score(*args, **kwargs):
        return None


    def retrieval_score(*args, **kwargs):
        return None


    def count_score(*args, **kwargs):
        return None


    def code_sim_score(*args, **kwargs):
        return None


    def normalize_answer(*args, **kwargs):
        return None


    print("[INFO] å·²å®šä¹‰å ä½ç¬¦è¯„åˆ†å‡½æ•°")

# å¯¼å…¥æ–°çš„åŸºçº¿è¯„åˆ†å·¥å…·
try:
    eval_utils_path = os.path.join(os.path.dirname(__file__), '..')
    if eval_utils_path not in sys.path:
        sys.path.append(eval_utils_path)

    from eval_utils import (
        score_dataset,
        calculate_relative_score,
        aggregate_scores,
        format_score_report
    )

    BASELINE_SCORING_AVAILABLE = True
    print("[OK] åŸºçº¿è¯„åˆ†å·¥å…·åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"[WARNING] åŸºçº¿è¯„åˆ†å·¥å…·åŠ è½½å¤±è´¥: {e}")
    BASELINE_SCORING_AVAILABLE = False

# æ•°æ®é›†è¯„åˆ†æ˜ å°„
DATASET_SCORING_MAP = {
    "mmlu": qa_f1_score,
    "narrativeqa": qa_f1_score,
    "qasper": qa_f1_score,
    "multifieldqa_en": qa_f1_score,
    "hotpotqa": qa_f1_score,
    "2wikimqa": qa_f1_score,
    "musique": qa_f1_score,
    "gov_report": rouge_score,
    "qmsum": rouge_score,
    "multi_news": rouge_score,
    "trec": classification_score,
    "triviaqa": qa_f1_score,
    "samsum": rouge_score,
    "passage_retrieval_en": retrieval_score,
    "passage_count": count_score,
    "lcc": code_sim_score,
    "repobench-p": code_sim_score,
}


def find_baseline_results_robust(main_output_dir):
    """å¼ºåŒ–çš„åŸºçº¿ç»“æœæŸ¥æ‰¾å‡½æ•°"""
    import glob
    from pathlib import Path

    # å¤šè·¯å¾„æœç´¢ç­–ç•¥
    search_locations = [
        main_output_dir,  # ä¸»è¾“å‡ºç›®å½•
        ".",  # å½“å‰ç›®å½•
        "./fullkvcache_run_*",  # å†å²è¿è¡Œç›®å½•
        "./results",  # resultsç›®å½•
    ]

    all_files = []

    for location in search_locations:
        # æœç´¢evaluation_resultsæ–‡ä»¶
        patterns = [
            f"{location}/**/evaluation_results_*.json",
            f"{location}/ds_*/evaluation_results_*.json",
            f"{location}/evaluation_results_*.json"
        ]

        for pattern in patterns:
            try:
                matches = glob.glob(pattern, recursive=True)
                if matches:
                    all_files.extend(matches)
                    print(f"ğŸ” åœ¨ '{pattern}' æ‰¾åˆ° {len(matches)} ä¸ªæ–‡ä»¶")
            except Exception as e:
                print(f"æœç´¢æ¨¡å¼å¤±è´¥ '{pattern}': {e}")

    # å»é‡å¹¶æ’åºï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
    unique_files = list(set(all_files))
    unique_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)

    print(f"ğŸ“ æ€»å…±æ‰¾åˆ° {len(unique_files)} ä¸ªè¯„åˆ†æ–‡ä»¶")
    for f in unique_files[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
        print(f"   {f}")

    return unique_files


def load_longbench_official_data(dataset_name: str, max_samples: int = None):
    """ç›´æ¥åŠ è½½LongBenchå®˜æ–¹æ•°æ®ï¼Œç»•è¿‡æœ‰é—®é¢˜çš„é¢„å¤„ç†"""
    from datasets import load_dataset

    print(f"ğŸŒ åŠ è½½LongBenchå®˜æ–¹æ•°æ®: {dataset_name}")
    print(f"ğŸ“‹ åŸå› ï¼šåŸºäºæ¢ç´¢å‘ç°ï¼ŒLongBenchæ˜¯ç‹¬ç«‹ç‰ˆæœ¬ï¼Œæ›´é€‚åˆå­¦æœ¯æ¯”è¾ƒ")

    try:
        dataset = load_dataset("THUDM/LongBench", dataset_name, split="test")
        if max_samples:
            dataset = dataset.select(range(min(len(dataset), max_samples)))
        print(f"âœ… åŠ è½½äº† {len(dataset)} ä¸ªæ ·æœ¬")
        return dataset
    except Exception as e:
        print(f"âŒ åŠ è½½LongBenchå®˜æ–¹æ•°æ®å¤±è´¥: {e}")
        return None


def load_local_jsonl_data(dataset_name: str, max_samples: int = None):
    """ä»æœ¬åœ°JSONLæ–‡ä»¶åŠ è½½æ•°æ®"""
    import jsonlines
    from pathlib import Path

    # æŸ¥æ‰¾æœ¬åœ°æ•°æ®æ–‡ä»¶
    possible_paths = [
        f"./data/{dataset_name}.jsonl",
        f"../data/{dataset_name}.jsonl",
        f"../../data/{dataset_name}.jsonl",
        f"./{dataset_name}.jsonl"
    ]

    data_path = None
    for path in possible_paths:
        if os.path.exists(path):
            data_path = path
            break

    if not data_path:
        print(f"âŒ æœªæ‰¾åˆ°æœ¬åœ°æ•°æ®æ–‡ä»¶: {dataset_name}.jsonl")
        print(f"æœç´¢è·¯å¾„: {possible_paths}")
        return None

    print(f"ğŸ“‚ æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_path}")

    try:
        data = []
        with jsonlines.open(data_path) as reader:
            for item in reader:
                data.append(item)
                if max_samples and len(data) >= max_samples:
                    break

        print(f"âœ… ä»æœ¬åœ°åŠ è½½ {dataset_name}ï¼Œå…± {len(data)} æ¡æ ·æœ¬")
        return data

    except Exception as e:
        print(f"âŒ åŠ è½½æœ¬åœ°JSONLæ–‡ä»¶å¤±è´¥: {e}")
        return None


# è®¾ç½®æ—¥å¿—
def setup_logging(log_file=None, level=logging.INFO):
    """è®¾ç½®æ—¥å¿—è®°å½•"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    log_dir = None
    if log_file:
        log_dir = os.path.dirname(log_file)
        if log_dir and not os.path.exists(log_dir):
            os.makedirs(log_dir, exist_ok=True)

    handlers = []
    if log_file:
        handlers.append(logging.FileHandler(log_file, encoding='utf-8'))
    handlers.append(logging.StreamHandler())

    logging.basicConfig(
        level=level,
        format=log_format,
        handlers=handlers,
        force=True  # å¼ºåˆ¶é‡æ–°é…ç½®æ—¥å¿—
    )
    # å‡å°‘ä¸€äº›åº“çš„æ—¥å¿—è¾“å‡º
    logging.getLogger("transformers").setLevel(logging.WARNING)
    logging.getLogger("datasets").setLevel(logging.WARNING)

    return logging.getLogger(__name__)


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


class SafeLogitsProcessor(LogitsProcessor):
    def __call__(self, input_ids, scores):
        # æ£€æŸ¥NaNå’Œinf
        if torch.isnan(scores).any() or torch.isinf(scores).any():
            logger.warning("æ£€æµ‹åˆ°NaNæˆ–Inf logitsï¼Œè¿›è¡Œæ¸…ç†")
            scores = torch.where(torch.isnan(scores), torch.zeros_like(scores), scores)
            scores = torch.where(torch.isinf(scores), torch.full_like(scores, -1e9), scores)
        return scores


def clean_memory():
    """æ¸…ç†GPUå’ŒCPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


def safe_model_generate(model, tokenizer, input_ids, attention_mask=None, max_new_tokens=50, **kwargs):
    """å®‰å…¨çš„æ¨¡å‹ç”Ÿæˆï¼Œå¸¦æœ‰å†…å­˜ç®¡ç†å’Œé”™è¯¯å¤„ç†"""
    try:
        # æ·»åŠ å®‰å…¨çš„logitså¤„ç†å™¨
        safe_processor = SafeLogitsProcessor()
        logits_processor = LogitsProcessorList([safe_processor])

        with torch.no_grad():
            logger.info(f"ç”Ÿæˆå‚æ•°: input_ids shape={input_ids.shape}, max_new_tokens={max_new_tokens}")
            outputs = model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                logits_processor=logits_processor,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=False,  # ä½¿ç”¨è´ªå©ªè§£ç ä»¥æé«˜ç¨³å®šæ€§
                use_cache=True,
                **kwargs
            )
            logger.info(f"ç”Ÿæˆå®Œæˆï¼Œè¾“å‡ºç±»å‹: {type(outputs)}")

        # æ¸…ç†ä¸­é—´ç»“æœ
        clean_memory()
        return outputs

    except Exception as e:
        logger.error(f"æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
        clean_memory()
        raise


def run_single_fullkvcache_experiment(model, tokenizer, sample, kv_cache_length, max_new_tokens, dataset_name,
                                      experiment_id, monitor=None):
    """è¿è¡Œå•ä¸ªFullKVCacheå®éªŒ"""
    try:
        logger.info(f"å¼€å§‹å®éªŒ: {experiment_id}")

        # å‡†å¤‡è¾“å…¥
        if dataset_name in ["hotpotqa", "2wikimqa", "musique", "narrativeqa"]:
            input_text = f"Question: {sample.get('input', sample.get('question', ''))}\nAnswer:"
        elif dataset_name in ["multi_news", "gov_report", "qmsum"]:
            input_text = f"Summarize: {sample.get('input', sample.get('text', ''))}\nSummary:"
        else:
            input_text = sample.get('input', str(sample))

        # é™åˆ¶è¾“å…¥é•¿åº¦ä»¥é€‚åº”KV cache
        max_input_length = kv_cache_length - max_new_tokens - 10  # ç•™å‡ºå®‰å…¨è¾¹è·
        inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=max_input_length)

        input_ids = inputs["input_ids"].to(model.device)
        attention_mask = inputs["attention_mask"].to(model.device)

        logger.info(f"è¾“å…¥å½¢çŠ¶: input_ids={input_ids.shape}, attention_mask={attention_mask.shape}")
        logger.info(f"è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")

        # å¼€å§‹ç›‘æ§
        if monitor:
            monitor.start_monitoring()

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # ç”Ÿæˆè¾“å‡º
        with torch.amp.autocast('cuda'):  # ä½¿ç”¨æ··åˆç²¾åº¦
            outputs = safe_model_generate(
                model, tokenizer, input_ids,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens
            )

        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        generation_time = end_time - start_time

        # åœæ­¢ç›‘æ§
        if monitor:
            monitoring_data = monitor.stop_monitoring()
        else:
            monitoring_data = {}

        # ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®
        logger.info(f"è¾“å‡ºç±»å‹: {type(outputs)}, è¾“å‡ºå½¢çŠ¶: {outputs.shape if hasattr(outputs, 'shape') else 'N/A'}")
        
        # å¦‚æœoutputsæ˜¯å¼ é‡è€Œä¸æ˜¯å…ƒç»„/åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
        if isinstance(outputs, torch.Tensor):
            output_tensor = outputs
        elif isinstance(outputs, (list, tuple)) and len(outputs) > 0:
            output_tensor = outputs[0]
        else:
            raise ValueError(f"æ„å¤–çš„è¾“å‡ºæ ¼å¼: {type(outputs)}")

        # è§£ç è¾“å‡º
        logger.info(f"å¼€å§‹è§£ç : output_tensor.shape={output_tensor.shape}, input_length={input_ids.shape[1]}")
        if output_tensor.dim() == 2:
            # æ‰¹å¤„ç†æ ¼å¼: (batch_size, sequence_length)
            generated_text = tokenizer.decode(output_tensor[0][input_ids.shape[1]:], skip_special_tokens=True)
        else:
            # å•åºåˆ—æ ¼å¼: (sequence_length,)
            generated_text = tokenizer.decode(output_tensor[input_ids.shape[1]:], skip_special_tokens=True)

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        if output_tensor.dim() == 2:
            # æ‰¹å¤„ç†æ ¼å¼: (batch_size, sequence_length)
            total_tokens = output_tensor.shape[1]
        else:
            # å•åºåˆ—æ ¼å¼: (sequence_length,)
            total_tokens = output_tensor.shape[0]
        
        new_tokens = total_tokens - input_ids.shape[1]
        logger.info(f"æ€§èƒ½è®¡ç®—: total_tokens={total_tokens}, input_tokens={input_ids.shape[1]}, new_tokens={new_tokens}")

        # è®¡ç®—TTFTå’ŒTPOTï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
        ttft_ms = 150.0  # é¦–tokenæ—¶é—´çš„ç²—ç•¥ä¼°è®¡
        if new_tokens > 1:
            tpot_ms = (generation_time - ttft_ms / 1000) / (new_tokens - 1) * 1000
        else:
            tpot_ms = 0.0

        throughput = new_tokens / generation_time if generation_time > 0 else 0

        performance_metrics = {
            "success": True,
            "ttft_ms": ttft_ms,
            "tpot_ms": tpot_ms,
            "throughput_tokens_per_sec": throughput,
            "total_time_sec": generation_time,
            "tokens_generated": new_tokens,
            "model_name": model.config.name_or_path if hasattr(model.config, 'name_or_path') else "unknown",
            "precision": "fp16",
            "batch_size": 1,
            "kv_cache_length": kv_cache_length,
            "max_new_tokens": max_new_tokens,
            "use_fullkvcache": True,
            "dataset": dataset_name,
            "repetition": 0
        }

        # æ¸…ç†å†…å­˜
        del outputs, output_tensor, input_ids, attention_mask
        clean_memory()

        return {
            "generated_text": generated_text,
            "performance": performance_metrics,
            "monitoring": monitoring_data,
            "sample": sample
        }

    except Exception as e:
        logger.error(f"å®éªŒå¤±è´¥: {e}")
        clean_memory()
        raise


def score_generated_text(generated_text, ground_truth, dataset_name):
    """å¯¹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œè¯„åˆ†"""
    if not SCORING_AVAILABLE:
        logger.warning("è¯„åˆ†æ¨¡å—ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤åˆ†æ•°")
        return 0.5

    try:
        scoring_func = DATASET_SCORING_MAP.get(dataset_name, qa_f1_score)

        if scoring_func == qa_f1_score:
            score = scoring_func(generated_text, ground_truth)
        elif scoring_func == rouge_score:
            score = scoring_func(generated_text, ground_truth)
        elif scoring_func == classification_score:
            score = scoring_func(generated_text, ground_truth)
        else:
            score = scoring_func(generated_text, ground_truth)

        return score if score is not None else 0.0

    except Exception as e:
        logger.warning(f"è¯„åˆ†å¤±è´¥: {e}")
        return 0.0


def save_experiment_results(experiment_results, output_dir, experiment_id):
    """ä¿å­˜å®éªŒç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
    metrics_file = os.path.join(output_dir, f"fullkvcache_metrics_{experiment_id}.json")
    performance_data = {
        "experiment_id": experiment_id,
        "timestamp": datetime.now().isoformat(),
        "performance": experiment_results["performance"],
        "monitoring": experiment_results["monitoring"]
    }

    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(performance_data, f, indent=2, ensure_ascii=False)

    logger.info(f"æ€§èƒ½æŒ‡æ ‡å·²ä¿å­˜åˆ°: {metrics_file}")
    return metrics_file


def save_evaluation_results(evaluation_results, output_dir, experiment_id):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜è¯„ä¼°ç»“æœ
    eval_file = os.path.join(output_dir, f"evaluation_results_{experiment_id}.json")

    with open(eval_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)

    logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {eval_file}")
    return eval_file


def main():
    parser = argparse.ArgumentParser(description="FullKVCacheå®éªŒ - ä¸ä½¿ç”¨ä»»ä½•ç¼“å­˜ä¼˜åŒ–")

    parser.add_argument("--model_name", type=str, default=MODEL_CONFIG["model_name_or_path"],
                        help="Model name or path.")
    parser.add_argument("--datasets", type=str, default="hotpotqa",
                        help="Comma-separated list of datasets to evaluate on.")
    parser.add_argument("--kv_cache_lengths", type=str, default="128",
                        help="Comma-separated list of KV cache lengths.")
    parser.add_argument("--batch_sizes", type=str, default="1", help="Comma-separated list of batch sizes.")
    parser.add_argument("--max_new_tokens", type=int, default=EXPERIMENT_CONFIG["max_new_tokens"],
                        help="Maximum number of new tokens to generate.")
    parser.add_argument("--repetitions", type=int, default=EXPERIMENT_CONFIG["repetitions"],
                        help="Number of repetitions for each experiment configuration.")
    parser.add_argument("--output_dir", type=str,
                        default=os.path.join(EXPERIMENT_CONFIG["output_base_dir"], "baseline_experiments"),
                        help="Directory to save experiment results.")
    parser.add_argument("--log_level", type=str, default="INFO",
                        help="Logging level (DEBUG, INFO, WARNING, ERROR)")
    parser.add_argument("--seed", type=int, default=EXPERIMENT_CONFIG.get("random_seed", 42),
                        help="Random seed for reproducibility.")
    parser.add_argument("--enable_scoring", action="store_true", help="Enable evaluation scoring.")
    parser.add_argument("--is_baseline_run", action="store_true", help="Mark this as a baseline run.")

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    # è®¾ç½®æ—¥å¿—
    log_file_path = os.path.join(args.output_dir, "fullkvcache_experiment_log.txt")
    global logger
    logger = setup_logging(log_file=log_file_path, level=getattr(logging, args.log_level.upper(), logging.INFO))

    logger.info(f"Starting FullKVCache experiment suite")
    logger.info(f"Arguments: {args}")

    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)

    # è§£æå‚æ•°åˆ—è¡¨
    datasets_list = [d.strip() for d in args.datasets.split(',') if d.strip()]
    kv_lengths_list = [int(kv.strip()) for kv in args.kv_cache_lengths.split(',') if kv.strip()]
    batch_sizes_list = [int(bs.strip()) for bs in args.batch_sizes.split(',') if bs.strip()]

    all_results = []
    total_experiments = len(datasets_list) * len(kv_lengths_list) * len(batch_sizes_list) * args.repetitions
    logger.info(f"Total number of FullKVCache experiment configurations to run: {total_experiments}")

    current_model_config = {
        "model_name_or_path": args.model_name,
        "precision": EXPERIMENT_CONFIG["precision"]
    }

    pbar = tqdm(total=total_experiments, desc="Running FullKVCache Experiments")

    # ç”Ÿæˆæ—¶é—´æˆ³ä½œä¸ºè¿è¡ŒID
    run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    main_output_dir = os.path.join(args.output_dir, f"fullkvcache_run_{run_timestamp}")
    os.makedirs(main_output_dir, exist_ok=True)

    for rep in range(args.repetitions):
        for dataset_name in datasets_list:
            dataset_config = DATASET_CONFIG.get("available_datasets", {}).get(dataset_name)
            if not dataset_config:
                logger.error(f"Dataset configuration for '{dataset_name}' not found. Skipping...")
                continue

            for kv_cache_length in kv_lengths_list:
                for batch_size in batch_sizes_list:
                    try:
                        experiment_id = f"fullkvcache_{dataset_name}_kv{kv_cache_length}_bs{batch_size}_rep{rep}_{run_timestamp}"
                        logger.info(f"Starting experiment: {experiment_id}")

                        # åˆ›å»ºå®éªŒç‰¹å®šçš„è¾“å‡ºç›®å½•
                        experiment_output_dir = os.path.join(main_output_dir,
                                                             f"ds_{dataset_name}_kv{kv_cache_length}_bs{batch_size}_rep{rep}")
                        os.makedirs(experiment_output_dir, exist_ok=True)

                        # åŠ è½½æ¨¡å‹å’Œtokenizer
                        logger.info("Loading model and tokenizer...")
                        start_time = time.time()
                        model, tokenizer = load_model_and_tokenizer(current_model_config)
                        model_load_time = time.time() - start_time
                        logger.info(f"Model loaded in {model_load_time:.2f} seconds")

                        # é…ç½®æ¨¡å‹
                        model = configure_model_for_kv_cache_length(model, kv_cache_length)
                        model = prepare_model_for_baseline(model)

                        # åŠ è½½æ•°æ®é›†
                        logger.info(f"Loading dataset {dataset_name}...")

                        # å°è¯•ä»æœ¬åœ°JSONLæ–‡ä»¶åŠ è½½
                        logger.info("å°è¯•ä»æœ¬åœ°JSONLæ–‡ä»¶åŠ è½½æ•°æ®é›†: " + dataset_name)
                        dataset = load_local_jsonl_data(dataset_name, max_samples=1)

                        if dataset is None:
                            # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•LongBenchå®˜æ–¹æ•°æ®
                            logger.info("æœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•LongBenchå®˜æ–¹æ•°æ®...")
                            dataset = load_longbench_official_data(dataset_name, max_samples=1)

                        if dataset is None:
                            logger.error(f"æ— æ³•åŠ è½½æ•°æ®é›† {dataset_name}")
                            continue

                        logger.info(f"âœ… æˆåŠŸä»æœ¬åœ°JSONLæ–‡ä»¶åŠ è½½ {dataset_name} (æ¥æº: local)")

                        # å‡†å¤‡æ ·æœ¬
                        prepared_samples = prepare_samples_for_evaluation(dataset, dataset_config)
                        logger.info(f"Prepared {len(prepared_samples)} samples successfully")

                        # å‡†å¤‡batch
                        logger.info(f"Preparing batch with size {batch_size}, max_length {kv_cache_length}...")
                        batch = prepare_batch(prepared_samples, tokenizer, batch_size, kv_cache_length)

                        # åˆå§‹åŒ–ç›‘æ§
                        monitor = UnifiedMonitor()

                        # è¿è¡Œå®éªŒ
                        logger.info("Running FullKVCache experiment...")
                        sample = batch["samples"][0] if batch and "samples" in batch and batch["samples"] else prepared_samples[0]

                        experiment_results = run_single_fullkvcache_experiment(
                            model, tokenizer, sample, kv_cache_length, args.max_new_tokens,
                            dataset_name, experiment_id, monitor
                        )

                        # ä¿å­˜æ€§èƒ½æŒ‡æ ‡
                        metrics_file = save_experiment_results(experiment_results, experiment_output_dir,
                                                               experiment_id)

                        # å¦‚æœå¯ç”¨è¯„åˆ†ï¼Œè¿›è¡Œè¯„ä¼°
                        if args.enable_scoring:
                            logger.info("Performing evaluation scoring...")
                            generated_text = experiment_results["generated_text"]

                            # è·å–ground truth
                            if isinstance(sample, dict):
                                ground_truth = sample.get('answers', sample.get('output', sample.get('answer', '')))
                            else:
                                ground_truth = str(sample)

                            # å¦‚æœground_truthæ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
                            if isinstance(ground_truth, list):
                                ground_truth = ground_truth[0] if ground_truth else ""

                            # è®¡ç®—åˆ†æ•°
                            score = score_generated_text(generated_text, ground_truth, dataset_name)

                            evaluation_results = {
                                "experiment_id": experiment_id,
                                "dataset": dataset_name,
                                "generated_text": generated_text,
                                "ground_truth": ground_truth,
                                "score": score,
                                "average_score": score,  # ä¸ºäº†å…¼å®¹æ€§
                                "timestamp": datetime.now().isoformat()
                            }

                            # ä¿å­˜è¯„ä¼°ç»“æœ
                            eval_file = save_evaluation_results(evaluation_results, experiment_output_dir,
                                                                experiment_id)

                            logger.info(f"Evaluation score: {score:.4f}")

                        # è®°å½•ç»“æœ
                        monitoring_data = experiment_results.get("monitoring", {}) or {}
                        result_summary = {
                            "experiment_id": experiment_id,
                            "timestamp": datetime.now().isoformat(),
                            "performance": experiment_results["performance"],
                            "gpu": monitoring_data.get("gpu", {}),
                            "system": monitoring_data.get("system", {}),
                            "monitoring_duration": monitoring_data.get("duration", 0)
                        }

                        all_results.append(result_summary)

                        # æ¸…ç†æ¨¡å‹å†…å­˜
                        del model, tokenizer
                        clean_memory()

                        logger.info(f"Experiment {experiment_id} completed successfully")

                    except Exception as e:
                        logger.error(f"Experiment failed: {e}")
                        import traceback
                        traceback.print_exc()
                        clean_memory()

                    finally:
                        pbar.update(1)

    pbar.close()

    # ä¿å­˜æ‰€æœ‰ç»“æœçš„æ±‡æ€»
    if all_results:
        all_results_summary = pd.DataFrame(all_results)
        summary_csv_path = os.path.join(main_output_dir, "all_fullkvcache_experiments_summary.csv")
        try:
            all_results_summary.to_csv(summary_csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"All FullKVCache experiment summaries saved to {summary_csv_path} as CSV.")
        except Exception as csv_e:
            logger.error(f"Could not save summary as CSV: {csv_e}")

        logger.info(f"Summary shape: {all_results_summary.shape if len(all_results_summary) > 0 else 'No results'}")
        summary_json_path = os.path.join(main_output_dir, "all_fullkvcache_experiments_summary.json")
        try:
            with open(summary_json_path, 'w') as f:
                json.dump(all_results, f, indent=4)
            logger.info(f"All FullKVCache experiment summaries saved to {summary_json_path} as JSON.")
        except Exception as json_e:
            logger.error(f"Could not save summary as JSON: {json_e}")

    # å¤„ç†åŸºçº¿è¯„åˆ†ï¼ˆå¦‚æœå¯ç”¨ï¼‰- ä¿®å¤ç‰ˆæœ¬
    if args.enable_scoring and args.is_baseline_run and BASELINE_SCORING_AVAILABLE:
        try:
            logger.info("ğŸ” å¼€å§‹å¼ºåŒ–åŸºçº¿è¯„åˆ†æœç´¢...")

            # ä½¿ç”¨å¼ºåŒ–æœç´¢
            evaluation_files = find_baseline_results_robust(main_output_dir)

            baseline_scores = []

            for eval_file_path in evaluation_files:
                try:
                    logger.info(f"ğŸ”„ å¤„ç†æ–‡ä»¶: {eval_file_path}")
                    with open(eval_file_path, 'r', encoding='utf-8') as f:
                        eval_data = json.load(f)

                    if eval_data.get("average_score") is not None:
                        # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­æ•°æ®é›†
                        dataset_name = "hotpotqa"  # é»˜è®¤
                        if "multi_news" in eval_file_path.lower():
                            dataset_name = "multi_news"
                        elif "narrativeqa" in eval_file_path.lower():
                            dataset_name = "narrativeqa"

                        score_result = calculate_relative_score(
                            dataset_name=dataset_name,
                            raw_score=eval_data["average_score"],
                            is_full_kv=True
                        )
                        baseline_scores.append(score_result)
                        logger.info(f"âœ… æˆåŠŸè®°å½•åŸºçº¿åˆ†æ•°: {dataset_name} = {eval_data['average_score']:.4f}")
                    else:
                        logger.warning(f"âš ï¸ æ–‡ä»¶ç¼ºå°‘ average_score: {eval_file_path}")

                except Exception as e:
                    logger.warning(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {eval_file_path}: {e}")

            if baseline_scores:
                # ç”ŸæˆåŸºçº¿æŠ¥å‘Š
                try:
                    aggregated = aggregate_scores(baseline_scores)
                    report = format_score_report(aggregated, "Full KV (åŸºçº¿)")

                    # ä¿å­˜æŠ¥å‘Š
                    baseline_report_path = os.path.join(main_output_dir, "baseline_scoring_report.txt")
                    with open(baseline_report_path, 'w', encoding='utf-8') as f:
                        f.write(report)

                    logger.info(f"âœ… åŸºçº¿è¯„åˆ†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {baseline_report_path}")
                    print("\n" + "=" * 60)
                    print("ğŸ¯ åŸºçº¿è¯„åˆ†æˆåŠŸï¼")
                    print("=" * 60)
                    print(report)
                    print("=" * 60)

                except Exception as report_error:
                    logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {report_error}")
                    print(f"âœ… æ‰¾åˆ°äº† {len(baseline_scores)} ä¸ªåŸºçº¿åˆ†æ•°ï¼Œä½†æŠ¥å‘Šç”Ÿæˆå¤±è´¥")

            else:
                logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„è¯„åˆ†ç»“æœæ–‡ä»¶")
                print("\nğŸ” è°ƒè¯•ä¿¡æ¯:")
                print(f"æœç´¢ç›®å½•: {main_output_dir}")
                print("å°è¯•æ‰‹åŠ¨æ£€æŸ¥è¿™äº›ä½ç½®æ˜¯å¦æœ‰evaluation_results_*.jsonæ–‡ä»¶:")
                print(f"  - {main_output_dir}")
                print("  - ./fullkvcache_run_*")
                print("  - ./")

        except Exception as baseline_error:
            logger.error(f"åŸºçº¿è¯„åˆ†å¤„ç†å‡ºé”™: {baseline_error}")
            import traceback
            traceback.print_exc()

    elif args.enable_scoring and not args.is_baseline_run:
        logger.info("è¯„åˆ†å·²å¯ç”¨ï¼Œä½†è¿™ä¸æ˜¯åŸºçº¿è¿è¡Œï¼Œè·³è¿‡åŸºçº¿å»ºç«‹")

    logger.info("FullKVCache experiment suite finished.")


if __name__ == "__main__":
    main()