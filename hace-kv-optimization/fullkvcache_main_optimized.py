#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆFullKVCacheå®éªŒä¸»ç¨‹åº
æŒ‰ç…§ç”¨æˆ·å»ºè®®ä¿®å¤çš„ç‰ˆæœ¬ï¼š
- æ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼Œå¾ªç¯å¤ç”¨
- ä¿®å¤HotpotQAæ•°æ®é›†åŠ è½½é—®é¢˜
- ä¼˜åŒ–GPUå†…å­˜ç®¡ç†
"""

import os
import sys
import json
import argparse
import logging
import gc
from datetime import datetime
from pathlib import Path
import torch

# å¯¼å…¥åŸæœ‰æ¨¡å— - å½“å‰ç›®å½•å°±æ˜¯hace-kv-optimization
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)
from hace_core.models.model_loader import load_model_and_tokenizer, prepare_model_for_baseline
from hace_core.data.dataset_loader import load_dataset_split, prepare_samples_for_evaluation
from hace_core import config

# å¯¼å…¥è¯„åˆ†åŠŸèƒ½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
try:
    # æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥eval_utils
    current_dir = os.path.dirname(os.path.abspath(__file__))
    if current_dir not in sys.path:
        sys.path.append(current_dir)
    from eval_utils import calculate_relative_score, aggregate_scores, format_score_report, score_dataset
    BASELINE_SCORING_AVAILABLE = True
    print("[OK] åŸºçº¿è¯„åˆ†å·¥å…·åŠ è½½æˆåŠŸ")
except ImportError as e:
    print(f"[WARNING] åŸºçº¿è¯„åˆ†å·¥å…·ä¸å¯ç”¨: {e}")
    BASELINE_SCORING_AVAILABLE = False

# å…¨å±€é…ç½®
EXPERIMENT_CONFIG = config.EXPERIMENT_CONFIG
DATASET_CONFIG = config.DATASET_CONFIG

logger = logging.getLogger(__name__)

def main():
    """ä¼˜åŒ–ç‰ˆä¸»å‡½æ•°ï¼šæ¨¡å‹åªåŠ è½½ä¸€æ¬¡"""
    
    # å‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–ç‰ˆFullKVCacheå®éªŒ")
    parser.add_argument("--model_name", type=str, default=EXPERIMENT_CONFIG["model_name_or_path"])
    parser.add_argument("--datasets", type=str, default="hotpotqa,multi_news")
    parser.add_argument("--kv_cache_lengths", type=str, default="128,256,512,1024,2048")
    parser.add_argument("--enable_scoring", action="store_true")
    parser.add_argument("--is_baseline_run", action="store_true")
    
    args = parser.parse_args()
    
    # è§£æå‚æ•°
    datasets_list = [d.strip() for d in args.datasets.split(',') if d.strip()]
    kv_lengths_list = [int(kv.strip()) for kv in args.kv_cache_lengths.split(',') if kv.strip()]
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    logger.info("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆFullKVCacheå®éªŒ")
    
    # ===== å…³é”®ä¼˜åŒ–1ï¼šåªåŠ è½½ä¸€æ¬¡æ¨¡å‹ =====
    logger.info("ğŸ“š åŠ è½½æ¨¡å‹ï¼ˆä»…ä¸€æ¬¡ï¼‰...")
    model_config = {
        "model_name_or_path": args.model_name,
        "precision": EXPERIMENT_CONFIG["precision"]
    }
    
    try:
        model, tokenizer = load_model_and_tokenizer(model_config)
        model = prepare_model_for_baseline(model)
        logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.model_name}")
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    all_results = []
    
    try:
        # ===== æ ¸å¿ƒå¾ªç¯ï¼šå¤ç”¨æ¨¡å‹ï¼Œåªé…ç½®KVé•¿åº¦ =====
        for dataset_name in datasets_list:
            logger.info(f"ğŸ“Š å¤„ç†æ•°æ®é›†: {dataset_name}")
            
            # è·å–æ•°æ®é›†é…ç½®
            dataset_options = DATASET_CONFIG.get("available_datasets", {}).get(dataset_name)
            if not dataset_options:
                logger.error(f"æ•°æ®é›†é…ç½®æœªæ‰¾åˆ°: {dataset_name}")
                continue
            
            for kv_len in kv_lengths_list:
                logger.info(f"âš™ï¸ é…ç½®KVé•¿åº¦: {kv_len}")
                
                # ===== å…³é”®ä¼˜åŒ–2ï¼šä¸é‡æ–°åŠ è½½æ¨¡å‹ï¼Œåªé…ç½®KVé•¿åº¦ =====
                with torch.no_grad():
                    if hasattr(model.config, 'max_position_embeddings'):
                        model.config.max_position_embeddings = kv_len
                    model.config.use_cache = True
                
                try:
                    # è¿è¡Œå•æ¬¡å®éªŒ
                    result = run_single_experiment(
                        model, tokenizer, dataset_name, 
                        dataset_options, kv_len, args
                    )
                    all_results.append(result)
                    logger.info(f"âœ… å®éªŒå®Œæˆ: {dataset_name}_kv{kv_len}")
                    
                except Exception as e:
                    logger.error(f"âŒ å®éªŒå¤±è´¥: {dataset_name}_kv{kv_len}, é”™è¯¯: {e}")
                    all_results.append({
                        "dataset": dataset_name,
                        "kv_length": kv_len,
                        "error": str(e),
                        "status": "failed"
                    })
    
    except KeyboardInterrupt:
        logger.info("âš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    
    finally:
        # ===== å…³é”®ä¼˜åŒ–3ï¼šæœ€åæ‰æ¸…ç†æ¨¡å‹ =====
        logger.info("ğŸ§¹ æ¸…ç†æ¨¡å‹èµ„æº...")
        del model, tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        logger.info("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    
    # å¤„ç†ç»“æœ
    process_results(all_results, args)
    logger.info("ğŸ‰ ä¼˜åŒ–ç‰ˆFullKVCacheå®éªŒå®Œæˆï¼")

def run_single_experiment(model, tokenizer, dataset_name, dataset_options, kv_length, args):
    """è¿è¡Œå•æ¬¡å®éªŒï¼ˆæ¨¡å‹å·²åŠ è½½ï¼‰"""
    
    experiment_id = f"optimized_{dataset_name}_kv{kv_length}_{datetime.now().strftime('%H%M%S')}"
    logger.info(f"å¼€å§‹å®éªŒ: {experiment_id}")
    
    # åŠ è½½æ•°æ®é›†
    try:
        dataset, dataset_source = load_dataset_with_fallback(dataset_name, dataset_options)
        logger.info(f"æ•°æ®é›†åŠ è½½æˆåŠŸ: {dataset_name} (æ¥æº: {dataset_source})")
    except Exception as e:
        raise Exception(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    
    # å‡†å¤‡æ ·æœ¬
    num_samples = min(10, len(dataset))  # å¿«é€Ÿæµ‹è¯•ï¼Œåªç”¨10ä¸ªæ ·æœ¬
    samples = prepare_samples_for_evaluation(dataset, dataset_name, num_samples=num_samples)
    
    # æ¨ç†
    predictions = []
    references = []
    
    logger.info(f"å¼€å§‹æ¨ç†ï¼Œæ ·æœ¬æ•°: {len(samples)}")
    
    for i, sample in enumerate(samples):
        try:
            # æå–è¾“å…¥å’Œå‚è€ƒç­”æ¡ˆ
            prompt = sample.get("prompt", sample.get("input", ""))
            reference = extract_ground_truth(sample, dataset_source)
            
            # æ¨ç†
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=kv_length)
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    do_sample=False,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # è§£ç 
            generated_text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            predictions.append(generated_text.strip())
            references.append(reference.strip() if reference else "")
            
        except Exception as e:
            logger.warning(f"æ ·æœ¬ {i} æ¨ç†å¤±è´¥: {e}")
            predictions.append("")
            references.append("")
    
    # è¯„åˆ†
    result = {
        "experiment_id": experiment_id,
        "dataset": dataset_name,
        "kv_length": kv_length,
        "num_samples": len(predictions)
    }
    
    if BASELINE_SCORING_AVAILABLE and predictions and references:
        try:
            raw_score = score_dataset(dataset_name, predictions, references)
            result["raw_score"] = raw_score
            logger.info(f"åŸå§‹è¯„åˆ†: {raw_score:.4f}")
            
            # è®¡ç®—ç›¸å¯¹è¯„åˆ†ï¼ˆå¦‚æœæ˜¯åŸºçº¿è¿è¡Œï¼‰
            if args.is_baseline_run:
                score_result = calculate_relative_score(dataset_name, raw_score, is_full_kv=True)
                result.update(score_result)
                logger.info(f"åŸºçº¿åˆ†æ•°å·²è®°å½•: {dataset_name} = {raw_score:.4f}")
            
        except Exception as e:
            logger.warning(f"è¯„åˆ†å¤±è´¥: {e}")
    
    return result

def load_dataset_with_fallback(dataset_name, dataset_options, split="test"):
    """åŠ è½½æ•°æ®é›†ï¼Œä¼˜å…ˆHFï¼Œå¤±è´¥æ—¶æœ¬åœ°JSONL"""
    try:
        # å°è¯•Hugging Face
        logger.info(f"å°è¯•ä»Hugging FaceåŠ è½½: {dataset_name}")
        dataset = load_dataset_split(dataset_options, split=split)
        return dataset, "huggingface"
    except Exception as hf_error:
        logger.warning(f"HFåŠ è½½å¤±è´¥: {hf_error}")
        try:
            # å›é€€åˆ°æœ¬åœ°JSONL
            data = load_local_jsonl(dataset_name)
            return create_simple_dataset(data), "local"
        except Exception as local_error:
            raise Exception(f"æ‰€æœ‰æ•°æ®æºåŠ è½½å¤±è´¥. HF: {hf_error}, Local: {local_error}")

def load_local_jsonl(dataset_name, data_dir="./data"):
    """ä»æœ¬åœ°JSONLåŠ è½½æ•°æ®"""
    file_path = Path(data_dir) / f"{dataset_name}.jsonl"
    if not file_path.exists():
        raise FileNotFoundError(f"æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    data = []
    with open(file_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    data.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    
    logger.info(f"ä»æœ¬åœ°åŠ è½½ {dataset_name}: {len(data)} æ¡æ ·æœ¬")
    return data

def create_simple_dataset(data):
    """åˆ›å»ºç®€å•æ•°æ®é›†å¯¹è±¡"""
    class SimpleDataset:
        def __init__(self, data):
            self.data = data
        def __len__(self):
            return len(self.data)
        def __getitem__(self, idx):
            return self.data[idx]
        def __iter__(self):
            return iter(self.data)
    return SimpleDataset(data)

def extract_ground_truth(sample, dataset_source):
    """æå–å‚è€ƒç­”æ¡ˆ"""
    if "answers" in sample:
        answers = sample["answers"]
        if isinstance(answers, list) and answers:
            return answers[0]
        elif isinstance(answers, str):
            return answers
    elif "answer" in sample:
        return sample["answer"]
    return ""

def process_results(all_results, args):
    """å¤„ç†å®éªŒç»“æœ"""
    if not all_results:
        logger.warning("æ²¡æœ‰å®éªŒç»“æœ")
        return
    
    # ä¿å­˜ç»“æœ
    output_file = f"optimized_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    logger.info(f"ç»“æœå·²ä¿å­˜: {output_file}")
    
    # ç”ŸæˆåŸºçº¿æŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨è¯„åˆ†ä¸”æ˜¯åŸºçº¿è¿è¡Œï¼‰
    if args.enable_scoring and args.is_baseline_run and BASELINE_SCORING_AVAILABLE:
        try:
            baseline_scores = [r for r in all_results if 'relative_score' in r]
            if baseline_scores:
                aggregated = aggregate_scores(baseline_scores)
                report = format_score_report(aggregated, "ä¼˜åŒ–ç‰ˆFull KVåŸºçº¿")
                
                report_file = f"baseline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                with open(report_file, 'w', encoding='utf-8') as f:
                    f.write(report)
                
                logger.info(f"åŸºçº¿æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
                print(report)
        except Exception as e:
            logger.error(f"åŸºçº¿æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ å®éªŒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 