{
  "experiment_id": "baseline_mmlu_kv1024_bs4_rep0_122506",
  "timestamp": "2025-06-08T12:30:22.834482",
  "performance": {
    "success": true,
    "ttft_ms": 2339.3683433532715,
    "tpot_ms": 1162.3867156458837,
    "throughput_tokens_per_sec": 0.8569038203988061,
    "total_time_sec": 298.7499809265137,
    "tokens_generated": 256,
    "model_name": "NousResearch/Llama-2-7b-hf",
    "precision": "fp16",
    "batch_size": 4,
    "kv_cache_length": 1024,
    "max_new_tokens": 256,
    "use_baseline": true,
    "dataset": "mmlu"
  },
  "gpu": {
    "total_devices": 1,
    "device_0": {
      "peak_memory_mb": 9220.62841796875,
      "average_memory_mb": 6059.710446263946,
      "sample_count": 2644
    },
    "total_peak_memory_mb": 9220.62841796875
  },
  "system": {
    "peak_cpu_percent": 28.3,
    "average_cpu_percent": 8.316666666666666,
    "peak_memory_percent": 45.0,
    "peak_memory_used_gb": 28.728973388671875,
    "sample_count": 294
  },
  "monitoring_duration": 298.7946436405182
}