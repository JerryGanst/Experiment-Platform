2025-06-18 15:48:04,798 - __main__ - INFO - Starting FullKVCache experiment suite with run name: fullkv_baseline_0周三/0/1_154759
2025-06-18 15:48:04,798 - __main__ - INFO - Arguments: Namespace(model_name='NousResearch/Llama-2-7b-hf', datasets='hotpotqa,multi_news', kv_cache_lengths='128,256,512,1024,2048', batch_sizes='1', max_new_tokens=100, repetitions=1, output_dir='results\\fullkv_baseline', log_level='INFO', seed=42, run_name='fullkv_baseline_0周三/0/1_154759', enable_scoring=True, is_baseline_run=True)
2025-06-18 15:48:04,798 - __main__ - INFO - Global EXPERIMENT_CONFIG being used: {'model_name_or_path': 'C:/Users/JerryGanst/mistral_models/7B-Instruct-v0.3', 'precision': 'fp16', 'multi_model_experiments': True, 'experiment_models': ['NousResearch/Llama-2-7b-hf', 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'mistralai/Mistral-7B-Instruct-v0.3'], 'memory_management': {'auto_max_memory': True, 'manual_max_memory': {0: '23000MB'}, 'memory_buffer_ratio': 0.05, 'force_no_cpu_offload': True}, 'datasets': ['mmlu', 'gsm8k', 'winogrande', 'arc_challenge', 'hellaswag', 'truthful_qa_mc'], 'dataset_subset_size': {'mmlu': 100, 'gsm8k': 100, 'winogrande': None, 'arc_challenge': None, 'hellaswag': None, 'truthful_qa_mc': None, 'pubmed_qa': 100, 'cais/mmlu-zh': 50}, 'kv_cache_lengths': [128, 256, 512, 1024, 2048], 'batch_sizes': [1, 4, 8], 'max_new_tokens': 256, 'repetitions': 3, 'h2o_enabled': True, 'h2o_ratios': [0.1, 0.2, 0.3], 'eviction_strategies': ['attention', 'time_decay', 'hybrid'], 'h2o_kv_cache_lengths': [128, 256, 512, 1024, 2048], 'cake_enabled': True, 'layer_allocation_strategies': ['uniform', 'adaptive', 'attention_based'], 'layer_analysis_configs': {'attention_pattern_analysis': True, 'layer_importance_scoring': True, 'dynamic_allocation': True}, 'cache_budgets': [0.5, 0.7, 0.9], 'cake_kv_cache_lengths': [128, 256, 512, 1024, 2048], 'head_level_optimization': False, 'head_analysis_enabled': False, 'head_selection_strategy': 'top_k', 'head_k_value': 10, 'output_base_dir': 'results', 'enable_monitoring': True, 'monitor_interval': 0.5}
2025-06-18 15:48:04,820 - __main__ - INFO - Random seed set to 42
2025-06-18 15:48:04,820 - __main__ - INFO - Total number of FullKVCache experiment configurations to run: 10
2025-06-18 15:48:04,821 - __main__ - INFO - Running FullKVCache: Rep 1/1, Dataset: hotpotqa, KV_Len: 128, Batch: 1
2025-06-18 15:48:05,008 - __main__ - INFO - Starting FullKVCache experiment: fullkvcache_hotpotqa_kv128_bs1_rep0_20250618_154805
2025-06-18 15:48:05,059 - __main__ - INFO - 实验前内存清理完成
2025-06-18 15:48:05,059 - hace_core.utils.unified_monitor - INFO - 发现1个GPU设备
2025-06-18 15:48:05,059 - hace_core.utils.unified_monitor - INFO - 统一监控器初始化完成，实验ID: fullkvcache_hotpotqa_kv128_bs1_rep0_20250618_154805
2025-06-18 15:48:05,060 - hace_core.utils.unified_monitor - INFO - 记录实验配置
2025-06-18 15:48:05,060 - __main__ - INFO - Loading model and tokenizer...
2025-06-18 15:48:05,060 - hace_core.models.model_loader - INFO - Loading model: NousResearch/Llama-2-7b-hf
2025-06-18 15:48:05,060 - hace_core.models.model_loader - INFO - GPU 0: 总显存 12281MB, 自动设置上限 11000MB (缓冲比例: 5.0%)
2025-06-18 15:48:05,060 - hace_core.models.model_loader - INFO - 使用精细显存控制: {0: '11000MB'}
2025-06-18 15:48:14,345 - accelerate.big_modeling - WARNING - Some parameters are on the meta device because they were offloaded to the disk.
2025-06-18 15:48:14,346 - hace_core.models.model_loader - INFO - Model loaded successfully with dtype: torch.float16
2025-06-18 15:48:14,346 - hace_core.models.model_loader - INFO - GPU 0 显存使用: 已分配 0.00GB, 已保留 0.00GB
2025-06-18 15:48:15,037 - hace_core.models.model_loader - INFO - Tokenizer loaded successfully
2025-06-18 15:48:15,037 - __main__ - INFO - 模型加载后GPU内存: 0.00GB
2025-06-18 15:48:15,037 - hace_core.models.model_loader - INFO - Configuring model for KV cache length: 128
2025-06-18 15:48:15,037 - hace_core.models.model_loader - INFO - Updated max_position_embeddings from 4096 to 4096
2025-06-18 15:48:15,038 - hace_core.models.model_loader - INFO - Model llama configured successfully for KV cache length: 128
2025-06-18 15:48:15,038 - hace_core.models.model_loader - INFO - Preparing model for baseline testing with default KV cache
2025-06-18 15:48:15,038 - __main__ - INFO - Loading dataset hotpotqa...
2025-06-18 15:48:15,038 - __main__ - INFO - 尝试从本地JSONL文件加载数据集: hotpotqa
2025-06-18 15:48:15,038 - __main__ - WARNING - ⚠️ 无法从本地加载 hotpotqa: ❌ 本地文件不存在: ../../data\hotpotqa.jsonl
2025-06-18 15:48:15,038 - __main__ - INFO - 回退到从Hugging Face加载数据集: hotpotqa
2025-06-18 15:48:15,039 - hace_core.data.dataset_loader - INFO - Loading dataset: THUDM/LongBench (subset: hotpotqa) - split: test
