#!/usr/bin/env python3
"""
Hugging Face æ¨¡å‹ä¸‹è½½å™¨
æ”¯æŒå¤šç§æ¨¡å‹çš„è‡ªåŠ¨ä¸‹è½½å’Œç¼“å­˜
"""

import os
import sys
from pathlib import Path
import argparse

def install_requirements():
    """å®‰è£…å¿…è¦çš„ä¾èµ–"""
    try:
        import transformers
        import torch
        print("âœ… ä¾èµ–å·²å®‰è£…")
    except ImportError:
        print("ğŸ“¦ å®‰è£…å¿…è¦ä¾èµ–...")
        os.system("pip install transformers torch tqdm accelerate")

def download_model(model_name, save_dir=None, cache_only=False):
    """
    ä¸‹è½½Hugging Faceæ¨¡å‹
    
    Args:
        model_name: æ¨¡å‹åç§° (å¦‚ 'meta-llama/Llama-2-7b-chat-hf')
        save_dir: ä¿å­˜ç›®å½• (å¯é€‰)
        cache_only: æ˜¯å¦åªç¼“å­˜åˆ°é»˜è®¤ä½ç½®
    """
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        print(f"ğŸš€ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        
        # é…ç½®ç¼“å­˜ç›®å½•
        if save_dir and not cache_only:
            cache_dir = save_dir
            os.makedirs(cache_dir, exist_ok=True)
        else:
            cache_dir = None
            
        print("ğŸ“¥ ä¸‹è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            trust_remote_code=True
        )
        
        print("ğŸ“¥ ä¸‹è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        if save_dir and not cache_only:
            print(f"ğŸ’¾ ä¿å­˜åˆ°: {save_dir}")
            tokenizer.save_pretrained(save_dir)
            model.save_pretrained(save_dir)
            
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Hugging Face æ¨¡å‹ä¸‹è½½å™¨")
    parser.add_argument("--model", "-m", default="meta-llama/Llama-2-7b-chat-hf", 
                        help="æ¨¡å‹åç§°")
    parser.add_argument("--save-dir", "-s", 
                        default=r"C:\Users\Administrator\llama_models",
                        help="ä¿å­˜ç›®å½•")
    parser.add_argument("--cache-only", action="store_true",
                        help="åªç¼“å­˜åˆ°é»˜è®¤ä½ç½®")
    parser.add_argument("--list-models", action="store_true",
                        help="æ˜¾ç¤ºå¯ç”¨æ¨¡å‹åˆ—è¡¨")
    
    args = parser.parse_args()
    
    if args.list_models:
        print("ğŸ¤— æ¨èçš„å¼€æºæ¨¡å‹:")
        models = [
            "meta-llama/Llama-2-7b-chat-hf",
            "meta-llama/Llama-2-13b-chat-hf", 
            "meta-llama/Meta-Llama-3-8B-Instruct",
            "meta-llama/Meta-Llama-3.1-8B-Instruct",
            "mistralai/Mistral-7B-Instruct-v0.3",
            "microsoft/DialoGPT-medium",
            "facebook/opt-6.7b",
            "EleutherAI/gpt-j-6b"
        ]
        for i, model in enumerate(models, 1):
            print(f"  {i}. {model}")
        return
    
    # å®‰è£…ä¾èµ–
    install_requirements()
    
    # ä¸‹è½½æ¨¡å‹
    success = download_model(args.model, args.save_dir, args.cache_only)
    
    if success:
        print(f"ğŸ‰ æˆåŠŸä¸‹è½½æ¨¡å‹: {args.model}")
        if not args.cache_only:
            print(f"ğŸ“ ä¿å­˜ä½ç½®: {args.save_dir}")
    else:
        print("ğŸ’¥ ä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œæ¨¡å‹åç§°")

if __name__ == "__main__":
    main() 