HACE KV缓存压缩优化框架实验设计方案

背景与目标
在大模型长上下文推理中，Key-Value (KV) 缓存占用的显存会随着输入长度急剧增长，成为制约因素
openreview.net
。已有研究提出多种KV缓存压缩方法来降低显存占用，同时尽量保持模型性能。例如，层级（layer-level）压缩方法如CAKE通过针对不同层的注意力模式自适应分配缓存，实现仅使用完整KV缓存的3.2%仍维持模型性能
openreview.net
；头部（head-level）压缩方法如HeadKV利用注意力头的重要性差异，在保留仅1.5%缓存的情况下达到原始97%的性能
openreview.net
。又如PyramidInfer等层级裁剪方法发现高层关键KV条目数量逐层递减，可按层筛选重要上下文，实现54%以上的KV内存压缩且几乎无性能损失
arxiv.org
。这些成果表明：并非所有层或注意力头的信息都同等重要，合理裁剪缓存具有巨大的优化潜力。 HACE（Hierarchical-Head Attention Cache Enhancement）框架正是基于上述观察提出，它融合了“层级压缩”与“头级压缩”两种模式，旨在根据不同层和不同注意力头的作用，智能选择性保留关键KV信息，以最大限度降低显存与KV缓存占用，同时保持输出质量不受明显影响。我们的实验设计将全面验证HACE在各类场景下的稳定性和有效性，包括不同上下文长度、模型架构、任务类型，以及与现有主流方法的对比。实验目标是在苛刻的长序列压力下，评估HACE是否能够在更高压缩率下依然保持高下游任务性能和推理速度，从而证明该融合策略的优越性。
上下文长度设置
我们选择五种上下文长度作为测试场景：4K、16K、64K、96K、128K tokens。这些长度覆盖从常规到极限的KV缓存负载范围，分别代表轻度、中等、高强度、超高强度和极端长上下文情形。例如4K可以视为常见输入长度，而128K已接近当前顶尖模型的支持上限
openreview.net
。在每种长度下，我们都将评估HACE框架的性能，以观察其在不同“压力场景”下的稳定性。特别地，长度逐步递增将测试HACE在KV缓存占用随序列增长而暴增时，能否有效地压缩缓存且不引致性能骤降。这一设计可帮助找出HACE在长上下文下的瓶颈，并验证其相对于基线方法在超长输入下的优势。
模型选择
实验采用多种主流大模型以保证结论的普适性和全面性。主要测试模型包括：
LLaMA-2-7B（7亿参数，Transformer架构） – 开源英文基座模型代表，具有典型的多层多头注意力结构（32个注意力头），上下文窗口约4K。
Mistral-7B（7亿参数） – LLaMA衍生模型，支持滑动窗口注意力机制，可高效处理长上下文，是长序列场景的重要参考。
OPT-13B（130亿参数） – Meta提出的GPT系列开源模型，代表较大规模的Transformer架构（注意力头数适中），用于验证方法对不同参数规模模型的适用性。
Qwen-14B（142亿参数） – 中文-多语言模型代表，拥有相对较多的注意力头，支持中英双语场景，用于测试HACE在不同语言环境下的效果。
上述模型覆盖了不同架构类型、参数规模和注意力头数量，并涵盖英语和中文等多语言环境。有的模型（如Mistral-7B）在架构上针对长序列做了优化，有的则采用标准Transformer结构。通过在这些模型上实验，我们可以评估HACE对于不同模型架构的通用性，以及在中英文等语言下对输出质量和压缩效果的影响。
任务与数据集
我们选取多样化的下游任务来全面评估HACE在实际应用中的性能表现：
LongBench基准任务：
neuropurrfectai.substack.com
 我们重点使用LongBench长上下文基准中的代表性数据集，包括 NarrativeQA（长篇叙事阅读理解问答）、Qasper（学术论文问答数据集，涉及NLP论文内容）以及 MultiFieldQA-en（多领域长文档问答，英文版）。这些任务需要模型在大段文本中进行理解、检索和推理，涵盖了摘要、问答、检索等多种类型，可用于测评压缩后模型的理解和推理能力是否保持。
Needle-in-a-Haystack测试：我们设计了“大海捞针”场景，即在极长的上下文（“干草堆”）中埋入一小段关键信息（“针”），然后提问与该关键信息相关的问题
arxiv.org
。这相当于一种极端检索任务：模型必须从海量无关信息中找出有用内容。通过该测试，可验证HACE在高压缩率下能否仍然保留和检索到关键语句，从而评估压缩策略的有效性（如是否丢弃了关键信息）。此前有工作将此用于验证压缩对模型检索能力的影响
arxiv.org
。
“Head Importance Variance (HIV) Test”：原创设计的头部重要性差异测试。该任务通过构造特殊输入或分析已有数据，量化不同注意力头对模型输出的相对贡献差异。例如，我们可能在上下文中插入针对特定头有偏好的信息片段，或利用多组具有不同模式的上下文，观察各注意力头在HACE压缩前后重要性排序的变化。该测试将产出每一层各注意力头的重要性指标，从而评估HACE的头部级别压缩是否影响某些注意力头的功能发挥，以及各头是否出现分工明显的专业化现象。
上述任务组合了开放域问答、专业文献理解、跨领域检索等场景，确保对HACE的评估既包括定量的指标（如准确率、F1等），又涵盖定性的观察（如模型是否定位到“针”、注意力头权重分布变化等）。数据集中的样本将按照每个任务各选取约100个代表性样本，以平衡评估效率和结果置信度。
控制变量策略
为确保比较的公平公正，我们在设计实验时严格控制变量，尽量排除非算法因素对结果的影响：
上下文长度：在对比实验中，同一组比较对象使用相同的输入上下文长度（例如都为16K或都为64K），以隔离长度因素带来的影响。我们按照预定的5种长度阶段分别进行实验，对应相同条件下比较HACE与各基线方法的表现。
解码与生成参数：所有模型在各实验中采用一致的文本生成设置，例如使用相同的随机种子、解码策略（如Greedy或相同的采样温度、top-k等参数）、生成长度限制等。这样可确保不同方法输出差异主要来自缓存压缩策略，而非生成随机性。
缓存压缩率：对比HACE与其他压缩方法时，在相同压缩率条件下比较性能。例如，我们预先设定若干典型缓存保留比例（如1%、2%等），让各方法均按照尽量接近的比例压缩，然后比较其效果差异。这避免了由于压缩程度不同导致的性能不具有可比性。
运行环境：所有实验在相同的硬件和软件环境下进行，包括相同的平台、相同版本的深度学习框架、相同的计算资源配置等。我们将在统一的环境中依次运行各方法，避免因硬件差异（如GPU性能、内存带宽）造成的速度或内存测量偏差。
测试样本集：对于每项对比，我们使用相同的一批测试输入。例如，在比较HACE与CAKE在NarrativeQA上的表现时，使用完全相同的一组问题和对应长文档。这保证评价指标具有可比性。如果需要多次试验取平均，我们也确保每轮使用的样本集一致或等价。
通过上述严格的控制变量策略，我们能够最大程度地保证对比实验的公正性。所有不同方法间的差异都可归因于其算法本身，而非外部因素，从而使得对HACE的评估结论更加可信。
对比方法与压缩率
为验证HACE框架的优越性，我们选择当前几种主流的KV缓存压缩方案作为对比基线，并在多个压缩率设定下进行全面比较：
CAKE（Cascading and Adaptive KV cache Eviction）
openreview.net
：ICLR 2025提出的层级自适应KV驱逐方法。它按层偏好分配缓存预算，并考虑注意力的时间动态，在仅保留3.2% KV缓存时即可保持模型性能，128K极长上下文下解码延迟加速达10倍
openreview.net
。我们将实现CAKE算法来对比其与HACE在不同压缩率下的性能曲线。
PyramidKV（如PyramidInfer方法）
arxiv.org
：ACL 2024提出的金字塔式KV缓存压缩，通过层级逐层筛选重要上下文实现高效推理。其核心思想是高层仅保留对未来影响最大的少量KV，从而减少预计算量，在不牺牲性能前提下节省超过54%的缓存显存
arxiv.org
。作为纯层级压缩的代表，我们将在相同压缩比下比较PyramidKV与HACE的内存节省和准确率损失情况。
HeadKV
openreview.net
：ICLR 2025提出的注意力头级KV缓存压缩方法。它识别并保留贡献最大的注意力头对应的KV信息，对检索+推理任务特别有效。在仅1.5%缓存占用下即可达到完整缓存97%的问答性能
openreview.net
。我们将把HeadKV作为纯头级优化的代表，与HACE进行对比，观察融合层级+头部策略是否带来更佳的压缩效果。
其他方法：根据需要，我们可能补充对比其它已知KV缓存优化策略，如SnapKV、基于量化的KV压缩、Token剪枝等（若这些方法在我们的环境中易于实现）。这些对比将确保HACE的评估具有全面性。不过主要分析集中在上述三种最新且效果领先的方法上。
压缩率设定：为了绘制性能-内存压缩率曲线，我们针对每种方法测试多个缓存保留比例，包括0.5%、0.8%、1%、2%、3%、5%等典型点。选择这些极低到略高的压缩率，是为了覆盖从超高压缩（仅千分之五保留）到中度压缩的范围，从而观察性能随压缩率的变化趋势。我们将记录在各压缩率下模型的关键指标（如准确率、速度等），绘制曲线比较HACE与基线方法的性能衰减轨迹。预期HACE的曲线在低压缩率区域依然保持较高的性能，不出现骤降，并能明显优于其它方法。同样重要的是，通过多点的测试，我们可以发现各方法的性能拐点（即性能大幅下降的临界压缩率），这将在后续的性能崩溃分析中讨论。
评价指标
我们从内存占用、效率、准确性、以及注意力行为等多个角度对比各方法。主要指标包括：
峰值显存占用（MB）：记录生成过程中GPU（或CPU）端由于KV缓存导致的最大内存使用。该指标直接反映内存优化效果，越低越好。在相同上下文长度下比较HACE与其他方法的显存峰值，可量化HACE的节省幅度。例如，我们将报告压缩前后显存占用差异，以及相对于不压缩时的降低百分比。
KV缓存压缩率（%）：即保留下来的KV缓存体积占原始完整缓存的比例。我们将按预设值（0.5%等）进行控制，并核实各方法实际达到的保留率。虽然这是实验的自变量之一，但报告实际压缩率有助于了解算法执行情况与目标值的偏差，并用于计算后续的效率比等复合指标。
解码吞吐量与延迟：用生成速度衡量模型推理效率，包括每秒生成token数（tokens/s）以及每个token的平均时延（ms/token）。我们将在不同压缩策略下测量这些值，以评估HACE是否带来推理加速。例如，在高压缩下由于缓存减少，内存访问和通信开销降低，期望看到吞吐量提高、延迟降低
openreview.net
。我们将比较各方法在不同上下文长度下的tokens/s和ms/token曲线，分析哪种方法在长序列下保持最高的解码效率。
下游任务性能：以任务相关指标评估输出质量，如准确率（QA任务是否答对问题）、F1（对于有部分匹配的回答得分）、ROUGE（摘要或生成内容与参考的匹配程度）等。每个数据集使用其主流评估指标。例如NarrativeQA我们记录答案准确率或ROUGE-L，Qasper记录精确匹配和F1，MultiFieldQA记录准确率等。此指标直接反映压缩后模型的功能保留程度：理想情况下，与不压缩时相比，性能下降应尽可能小。
输出一致性与正确性：为了细致评估模型生成质量，我们还将检查输出内容的一致性。这包括比较压缩与未压缩情况下模型生成文本是否在事实内容、逻辑一致性上保持一致，以及有无因压缩导致的遗漏或错误。部分检查可通过自动度量（如与参考答案比较差异）完成，但对复杂开放生成，我们可能进行人工评估来判断输出是否正确传达了原意。此步骤确保压缩没有引入明显的语义偏差或关键内容缺失。
Head Specialization Score（头部专业化得分）：我们引入该自定义指标以量化不同注意力头在压缩过程中的作用差异。具体而言，我们将统计在推理过程中，各层的每个注意力头保留了多少关键KV信息或在注意力权重中占多大比重，然后计算其分布的方差或熵等指标。例如，高专业化意味着少数头承担主要信息（分布不均匀，方差大），低专业化则多数头平均分担（分布均匀）。通过比较HACE与其他方法的头部专业化得分，可评估HACE是否促进了注意力头的分工（例如让某些头专注于关键信息）抑或保持了均衡。如果HACE显著提高了该得分，说明其充分利用了注意力头的异质性来压缩缓存。
Cache Efficiency Ratio（缓存效率比）：这是我们定义的复合评价指标，用以综合考察性能 / 缓存占用的权衡。计算方式为在某一压缩率下的下游任务性能得分，除以该压缩率（或剩余缓存百分比）。直观上，它表示单位缓存带来的性能：在保留更少缓存的同时还能取得高性能的方法将获得更高的比值。我们将在不同方法、不同压缩率下计算该比值。例如，如果一种方法用2%的缓存达到90%的性能，其效率比=90/2=45；而另一方法用同样2%缓存只达到80%性能，则比值=40，前者效率更高。通过这一指标可以量化各方法对缓存利用的有效性，直观展示哪种方法在节省内存的同时仍维持了较高的性能产出。
综合以上指标，我们将从内存节省、速度提升、任务表现、注意力机制影响等方面，全方位比较HACE与基线方法。这些数据将以表格和曲线形式展示，并在结果分析中加以讨论。
扩展实验
除了上述主要实验外，我们还设计了一系列扩展实验来进一步深入分析HACE框架的行为和特性：
消融实验（Ablation Study）
我们将对HACE框架进行消融拆解，以评估层级压缩和头部压缩两种组件各自的贡献。具体设置包括：
HACE 完整版：启用层级和头部注意力模式优化的完整算法，即标准HACE，用作对照。
仅层级优化：保留HACE中的层级（按层分配缓存）策略，但关闭头部级别的自适应机制。这样可观察没有头部精细压缩时的性能。
仅头部优化：相反地，仅应用注意力头级的压缩策略，各层不进行差异化处理（或采用平均分配给层）。用于评估无层级优化时头部策略的效果。
通过上述三种变体在相同实验条件下的比较，我们将了解：1）单独的层级优化与单独头部优化，各自能达到何种程度的内存节省和性能表现；2）两者结合（HACE完整版）是否产生协同增益。我们预期HACE完整方案优于任一单独变体，若实验验证了这一点，将支持“层级+头部融合”的有效性。此外，消融实验的结果还能帮助定位HACE中最关键的模块：例如如果仅头部优化性能接近完整HACE，说明头部策略贡献更大；反之亦然。
“性能崖值”分析（Performance Cliff Analysis）
该分析旨在找出各方法在极端压缩下性能崩溃的临界点。我们将逐步降低KV缓存保留比例，观察模型性能的变化。当性能指标（如准确率）降至不可接受范围或出现陡降时，即认为到达“性能悬崖”。具体而言，在之前设定的0.5%、0.8%、1%、2%、3%、5%压缩率基础上，如有需要我们还可测试介于0.5%和1%之间的更细粒度点，以定位拐点。例如，某基线方法可能在压缩率低于1%时准确率骤降50%，而HACE也许可坚持到0.5%仍保持较高性能。这些阈值对于实际应用很重要：它告诉我们每种算法在多大内存节省下还能可靠工作。我们会将各方法的临界压缩率及对应性能下降幅度汇总成表。一旦发现某方法的性能拐点，我们还将分析其原因（例如注意力权重分布剧变、关键token被移除等），从而为理解不同压缩策略的极限提供依据。
注意力头重要性热力图（Head Importance Heatmap）
为了直观展示HACE在不同层级和注意力头上的缓存分配格局，我们计划绘制层×头二维热力图。横轴为Transformer的层（如第1层到第n层），纵轴为每层内的注意力头索引，热力图的颜色强度表示某指标的大小。我们可以采用以下两种可视化方式：
缓存保留分布图：颜色表示在应用压缩后，每一层的每个头所保留的KV条目数占该层原始KV的比例。这样可以看出HACE倾向于在哪些层、哪些头保留更多信息。例如，如果热图显示某几层的关键头亮度高，表示HACE认为它们重要，分配了较多缓存预算。
头重要性差异图：颜色表示每个头对模型最终输出的重要性分数（可由注意力累积权重或对下游任务损失的影响度量得到）。通过比较HACE与其他方法的热力图，可观察到HACE是否让重要的信息集中到少数头上。例如，与基线方法相比，HACE可能呈现更明显的“热点”区域（少数头颜色深），而其他方法可能色块分布较为平均。
热力图分析有助于回答：HACE是否改变了模型内部的注意力模式分配？具体哪些层和头最关键？这些图将作为定性结果插入实验报告中，并配以文字说明突出发现。例如，我们可能指出“HACE的热力图显示出高层后几头承担了主要信息存储任务，这与PyramidKV专注高层的重要token策略一致，同时低层部分头几乎完全裁剪，表明HACE在低层就过滤了冗余信息”。这种可视化能深入揭示HACE的工作机制，为读者提供直观理解。
资源需求与时间规划
实验资源：本次实验将在一台配备Intel i7-13700KF CPU、64GB DDR5内存的计算机上进行。由于涉及大模型推理，我们预计还需要高性能GPU支持（例如NVIDIA 3090/4090或A100等，至少24GB显存），用于加速长序列推理并测量GPU显存占用。软件环境方面，使用最新的深度学习框架（如PyTorch >= 2.0）以及相应的推理优化库（如FlashAttention等）以支持长上下文高效推理
openreview.net
。在运行过程中，我们将利用分批次处理和缓存复用等策略提升效率。例如，对每种条件以100条样本为一批进行推理评测，合理设置batch size（在显存允许范围内尽可能批处理，以提高GPU利用率）和缓存管理策略（如逐步扩展上下文测试时重复使用模型实例，以减少加载开销）。 时间计划：我们预估在约2周内完成全部主要实验。第一周主要投入在环境配置、模型部署和初步测试，确保HACE和各基线方法正确实现且参数设置合理。随后按场景逐步展开实验，例如：
第1–3天：小规模样本试运行，调试HACE压缩模块，在4K和16K上下文下验证输出正确性。
第4–7天：扩展到LongBench任务的系统测试，完成不同模型、不同压缩率的主要性能数据采集（重点关注64K及以下长度场景，以便尽早得到常规结论）。
第8–10天：开展超长上下文（96K、128K）实验，以及Needle-in-a-Haystack专项测试。这部分可能耗时较长，我们将合理安排在夜间连续运行，并监控显存占用避免溢出。
第11–14天：进行扩展实验（消融、性能拐点分析、热力图生成等）、数据后处理和结果分析。我们将多次运行消融实验以取得稳定均值，并生成曲线和图表。
在实验过程中，我们设置自动化脚本定期保存模型输出和日志，包括每次运行的配置、随机种子、性能指标统计等，确保所有结果可溯源且易于复现。关键阶段会保存中间检查点（如不同压缩率下的模型状态快照，重要输出案例等），防止长时间运行中断或意外导致的数据丢失。所有实验结果和代码将集中管理并备份，以方便后续撰写论文时调用。通过严密的计划与日志记录，我们有信心在预定时间内完成实验并得到可靠结论，为HACE框架的有效性提供翔实的实证支持。
# HACE KV 缓存压缩实验设计方案

> **目标**：在保证生成质量基本不变的前提下，最大限度压缩 KV 缓存，降低显存占用并提升推理速度。  
> **方法**：将层级（CAKE 风格）与注意力头级（HeadKV 风格）压缩融合，形成 **HACE** 框架，并与多种基线系统对比。

---

## 1. 实验维度与取值

| 维度 | 具体取值 | 设计目的 & 备注 |
|------|----------|----------------|
| **上下文长度** | 4K / 16K / 64K / 96K / 128K | 从轻负载到极限负载的五种“路况” |
| **模型** | LLaMA-2-7B<br>Mistral-7B<br>OPT-13B<br>Qwen-14B | 覆盖不同架构、规模与语言 |
| **缓存保留率** | 0.5 % / 0.8 % / 1 % / 2 % / 3 % / 5 % | 绘制性能-内存曲线，定位“性能崖值” |
| **方法对比** | HACE（完整）<br>HACE-LayerOnly<br>HACE-HeadOnly<br>CAKE<br>PyramidKV<br>HeadKV | 融合策略 vs. 单维策略 vs. 现有 SOTA |
| **任务 / 数据集** | NarrativeQA<br>Qasper<br>MultiFieldQA-en<br>Needle-in-a-Haystack<br>Head Importance Variance (HIV) Test | 长阅读理解 + 检索针 + 头差异诊断 |
| **样本数** | 每任务 100 例 | 控制方差，2 周内完成 |
| **硬件 / 环境** | i7-13700KF + 64 GB DDR5<br>单卡 ≥ 24 GB GPU<br>PyTorch ≥ 2.0 (Flash-Attention) | 统一环境；显存峰值用 `torch.cuda.max_memory_allocated` |

---

## 2. 主要评估指标

| 类别 | 指标 | 记录方式 & 解释 |
|------|------|----------------|
| **资源** | 峰值 GPU 显存 (MB) | `torch.cuda.max_memory_allocated()` |
| | 实际 KV 压缩率 (%) | 保留条目 ÷ 原条目 |
| **速度** | 吞吐量 (tokens/s)<br>时延 (ms/token) | 端到端计时；长序列期望近似恒定 |
| **质量** | NarrativeQA — ROUGE-L<br>Qasper — EM / F1<br>MultiFieldQA — Acc | 跌幅 ≤ 3 % 为目标 |
| **一致性** | 输出差分 + 人工评估 | 检查事实/逻辑一致性 |
| **注意力行为** | Head Specialization Score | 头专业化程度 |
| | Cache Efficiency Ratio = 任务得分 ÷ 压缩率 | 单位缓存性能收益 |

---

## 3. 消融 & 扩展实验

* **消融实验**  
  * **HACE-LayerOnly**：仅层级压缩  
  * **HACE-HeadOnly**：仅头级压缩

* **性能崖值分析**  
  * 逐步降低保留率，记录性能骤跌临界点

* **Head Importance Heatmap**  
  * 层 × 头二维热力图，颜色 = ↑KV 保留或 ↑重要性得分

---

## 4. 时间规划（2 周）

| 周 | 任务 |
|----|------|
| **Week 1** | 环境 + baseline 复现 → LLaMA-2-7B / Mistral-7B 主干实验 |
| **Week 2** | 超长上下文 (96K/128K) + 消融 + 诊断任务 → 结果可视化、数据汇总 |

---

## 5. 批处理脚本参数示例

```python
for model in ["llama2-7b", "mistral-7b"]:
    for length in [4096, 16384, 65536, 98304, 131072]:
        for rate in [0.005, 0.008, 0.01, 0.02, 0.03, 0.05]:
            run_eval(
                model=model,
                context_len=length,
                retention=rate,
                method="HACE",
                tasks=["narrativeqa", "qasper", "multifieldqa"]
            )
```

## Running Experiments

Use the Python script `run_experiment.py` located in the
`Research Framework for Optimizing Head-level KV Cache Based on CAKE` directory
to launch experiments. The script is platform independent and replaces the old
`run_experiment.ps1` helper.

```bash
python -m "Research Framework for Optimizing Head-level KV Cache Based on CAKE".run_experiment --help
